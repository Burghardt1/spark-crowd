%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}





\title{spark-crowd Documentation}
\date{Nov 07, 2018}
\release{0.2.1}
\author{Enrique G. Rodrigo \and Juan A. Aledo \and Jose A. Gamez}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\maketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


Learning from crowdsourced Big Data


\bigskip\hrule\bigskip
Learning from crowdsourced data imposes new challenges
in the area of machine learning. This package, \sphinxtitleref{spark-crowd}, helps practitioners
when dealing with this kind of data at scale, using Apache Spark.

\noindent\sphinxincludegraphics{{illustration}.png}

The main features of \sphinxtitleref{spark-crowd} are the following:
\begin{itemize}
\item {} 
It implements well-known methods for learning from crowdsourced labeled data.

\item {} 
It is suitable for working with both large and small datasets.

\item {} 
It uses Apache Spark, which allows the code to run in different environments, from a computer to a multi-node cluster.

\item {} 
It is suitable both for research and production environments.

\item {} 
It provides an easy to use API, allowing the practitioner to start using the library in minutes.

\item {}
It is licensed under the \sphinxhref{https://opensource.org/licenses/MIT/}{MIT license}.

\end{itemize}

See the {\hyperref[\detokenize{usage/quickstart:quickstart}]{\sphinxcrossref{\DUrole{std,std-ref}{Quick Start}}}} to started using \sphinxtitleref{spark-crowd}






\chapter{Quick Start}
\label{\detokenize{usage/quickstart:quick-start}}\label{\detokenize{usage/quickstart:quickstart}}\label{\detokenize{usage/quickstart::doc}}
You can easily start using \texttt{spark-crowd} package through our \sphinxhref{https://www.docker.com/}{docker} image or through \sphinxhref{https://spark-packages.org/}{Spark Packages}.
See {\hyperref[\detokenize{usage/installation:installation}]{\sphinxcrossref{\DUrole{std,std-ref}{Installation}}}}, for all the installation alternatives (such as how to add the package as a dependency in your project).


\section{Start with our docker image}
\label{\detokenize{usage/quickstart:start-with-our-docker-image}}
The quickest way to try out the package is through the
\sphinxhref{https://hub.docker.com/r/enriquegrodrigo/spark-crowd/}{provided docker image} with the latest version of
the package, as you do not need to install any other software (apart from docker).

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker pull enriquegrodrigo/spark\PYGZhy{}crowd
\end{sphinxVerbatim}

Thanks to it, you can run the examples provided along with the
\sphinxhref{https://github.com/enriquegrodrigo/spark-crowd}{package}. For example,
to run \sphinxtitleref{DawidSkeneExample.scala} we can use:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker run \PYGZhy{}\PYGZhy{}rm \PYGZhy{}it \PYGZhy{}v \PYG{k}{\PYGZdl{}(}\PYG{n+nb}{pwd}\PYG{k}{)}/:/home/work/project enriquegrodrigo/spark\PYGZhy{}crowd DawidSkeneExample.scala
\end{sphinxVerbatim}

You can also open a spark shell with the library preloaded.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker run \PYGZhy{}\PYGZhy{}rm \PYGZhy{}it \PYGZhy{}v \PYG{k}{\PYGZdl{}(}\PYG{n+nb}{pwd}\PYG{k}{)}/:/home/work/project enriquegrodrigo/spark\PYGZhy{}crowd
\end{sphinxVerbatim}

By doing that, you can test your code directly. You will not benefit from the distributed execution of Apache Spark, but you are still
able to use the algorithms with medium-sized datasets (since docker can use several cores in your machine).


\section{Start with Spark Packages}
\label{\detokenize{usage/quickstart:start-with-spark-packages}}
If you have an installation of \sphinxhref{https://spark.apache.org/}{Apache Spark}, you can open a \sphinxtitleref{spark-shell} with
our package pre-loaded using:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
spark\PYGZhy{}shell \PYGZhy{}\PYGZhy{}packages com.enriquegrodrigo:spark\PYGZhy{}crowd\PYGZus{}2.11:0.2.1
\end{sphinxVerbatim}

Likewise, you can submit an application to your cluster that uses \sphinxtitleref{spark-crowd} using:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
spark\PYGZhy{}submit \PYGZhy{}\PYGZhy{}packages com.enriquegrodrigo:spark\PYGZhy{}crowd\PYGZus{}2.11:0.2.1 application.scala
\end{sphinxVerbatim}

To use this option you do not need to have a cluster of computers, you may also execute the code from
your local machine because Apache Spark can be installed locally. For more information on how to install
Apache Spark, please refer to its \sphinxhref{https://spark.apache.org/}{homepage}. 


\section{Basic usage}
\label{\detokenize{usage/quickstart:basic-usage}}
Once you have chosen a procedure to run the package, you have to import the method
that you want to use as well as the types for your data, as you can see below:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.methods.DawidSkene}
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.types.MulticlassAnnotation}

\PYG{k}{val} \PYG{n}{exampleFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}examples/data/multi\PYGZhy{}ann.parquet\PYGZdq{}}

\PYG{k}{val} \PYG{n}{exampleData} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{exampleFile}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{MulticlassAnnotation}\PYG{o}{]}

\PYG{c+c1}{//Applying the learning algorithm}
\PYG{k}{val} \PYG{n}{mode} \PYG{k}{=} \PYG{n+nc}{DawidSkene}\PYG{o}{(}\PYG{n}{exampleData}\PYG{o}{)}

\PYG{c+c1}{//Get MulticlassLabel with the class predictions}
\PYG{k}{val} \PYG{n}{pred} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getMu}\PYG{o}{(}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{MulticlassLabel}\PYG{o}{]}

\PYG{c+c1}{//Annotator precision matrices}
\PYG{k}{val} \PYG{n}{annprec} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getAnnotatorPrecision}\PYG{o}{(}\PYG{o}{)}
\end{sphinxVerbatim}

You can find a description of the code below:
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
First the method and the type are imported, in this case \sphinxcode{\sphinxupquote{DawidSkene}} and \sphinxcode{\sphinxupquote{MulticlassAnnotation}}. The type is needed
as the package API only accepts typed datasets for the annotations. 

\item {} 
Then the data file (provided with the package) is loaded. It contains annotations for different examples. As you can see,  
the example uses the method \sphinxcode{\sphinxupquote{as}} to convert the Spark DataFrame in a typed Spark Dataset (with type \sphinxcode{\sphinxupquote{MulticlassAnnotation}}).

\item {} 
To execute the model and obtain the result you can use the model name directly.
This function returns a \sphinxcode{\sphinxupquote{DawidSkeneModel}}, which includes several methods to obtain results from the algorithm.

\item {} 
The method \sphinxcode{\sphinxupquote{getMu}} returns the ground truth estimations made by the model.

\item {} 
We use \sphinxcode{\sphinxupquote{getAnnotatorPrecision}} to obtain the annotator quality calculated by the model.

\end{enumerate}

You can consult the models implemented in this package in {\hyperref[\detokenize{package/methods:methods}]{\sphinxcrossref{\DUrole{std,std-ref}{Methods}}}}, where you can find a link to the
original article for the algorithm.


\chapter{Installation}
\label{\detokenize{usage/installation:installation}}\label{\detokenize{usage/installation:id1}}\label{\detokenize{usage/installation::doc}}
There are three alternatives to use the package in your own software:
\begin{itemize}
\item {} 
Using the package directly from Spark Packages 

\item {} 
Adding it as a dependency to your project through Maven central.

\item {} 
Compiling the source code and using the \sphinxcode{\sphinxupquote{jar}} file.

\end{itemize}

Alternatively, if you just want to execute simple scala scripts locally,
you can use the provided docker image as explained in {\hyperref[\detokenize{usage/quickstart:quickstart}]{\sphinxcrossref{\DUrole{std,std-ref}{Quick Start}}}}


\section{Using Spark Packages}
\label{\detokenize{usage/installation:using-spark-packages}}
The easiest way of using the package is through \sphinxhref{https://spark-packages.org/}{Spark Packages}, as you only need to add the package in the command line when running your
application:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
spark\PYGZhy{}submit \PYGZhy{}\PYGZhy{}packages com.enriquegrodrigo:spark\PYGZhy{}crowd\PYGZus{}2.11:0.2.1 application.scala
\end{sphinxVerbatim}

You can also open a \sphinxtitleref{spark-shell} using:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
spark\PYGZhy{}shell \PYGZhy{}\PYGZhy{}packages com.enriquegrodrigo:spark\PYGZhy{}crowd\PYGZus{}2.11:0.2.1
\end{sphinxVerbatim}


\section{Adding \texttt{spark-crowd} as a dependency}
\label{\detokenize{usage/installation:adding-it-as-a-dependency}}
In addition to Spark Packages, the library is also in Maven Central, so you can add it as a dependency in your scala project.
For example, in \sphinxstyleemphasis{sbt} you can add the dependency as shown below:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{libraryDependencies} \PYG{o}{+=} \PYG{l+s}{\PYGZdq{}com.enriquegrodrigo\PYGZdq{}} \PYG{o}{\PYGZpc{}\PYGZpc{}} \PYG{l+s}{\PYGZdq{}spark\PYGZhy{}crowd\PYGZdq{}} \PYG{o}{\PYGZpc{}} \PYG{l+s}{\PYGZdq{}0.2.1\PYGZdq{}}
\end{sphinxVerbatim}

This allows you to use the methods inside your Apache Spark projects.


\section{Compiling the source code}
\label{\detokenize{usage/installation:compiling-the-source-code}}
To build the package using \sphinxstyleemphasis{sbt} you can use the following command inside the \texttt{spark-crowd} folder:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
sbt package
\end{sphinxVerbatim}

It generates a compiled \sphinxcode{\sphinxupquote{jar}} file that you can add to your project.


\chapter{Design and architechture}
\label{\detokenize{package/design:design-and-architechture}}\label{\detokenize{package/design::doc}}
The package design can be found in the figure below.

\noindent\sphinxincludegraphics{{package}.png}

Although the library contains several folders, the only important ones for the users
are the \sphinxcode{\sphinxupquote{types}} folder, and the \sphinxcode{\sphinxupquote{methods}}. The other folders contain auxiliary
functions for some of the methods. Specifically, it is interesting to explore the data types, as
they are essential to understand how the package works, as well as the common interface of
the methods.


\section{Data types}
\label{\detokenize{package/design:data-types}}
The package provides types for annotation datasets and ground truth datasets, as they usually follow
the same structure. These types are used in all the methods so you need to convert
your annotations dataset to the correct format accepted by the algorithm.

There are three types of annotations that the package supports for which we provide Scala case classes,
making it possible to detect errors at compile time when using the algorithms:
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{BinaryAnnotation}}: a dataset of this type provides three columns: 
\begin{itemize}
    \item The \sphinxcode{\sphinxupquote{example}} column (i.e the example for which the annotation is made). 
    \item The \sphinxcode{\sphinxupquote{annotator}} column (representing the annotator that made the annotation).
    \item The \sphinxcode{\sphinxupquote{value}} column, (with the value of the annotation, that can take value 0 or 1).
\end{itemize}

\item {} 
\sphinxcode{\sphinxupquote{MulticlassAnnotation}}: The difference from \sphinxcode{\sphinxupquote{BinaryAnnotation}} is that the value column can
take more than two values, in the range from 0 to the total number of values.

\item {} 
\sphinxcode{\sphinxupquote{RealAnnotation}}: In this case, the value column can take any numeric value.

\end{itemize}

You can convert an annotation dataframe with columns: \sphinxcode{\sphinxupquote{example}}, \sphinxcode{\sphinxupquote{annotator}} 
and \sphinxcode{\sphinxupquote{value}} to a typed dataset with the following instruction:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{val} \PYG{n}{typedData} \PYG{k}{=} \PYG{n}{untypedData}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{RealAnnotation}\PYG{o}{]}
\end{sphinxVerbatim}

In the case of labels, we provide 5 types of labels, 2 of which are probabilistic. The three non probabilistic
types are:
\begin{itemize}
\item {} 
  \sphinxcode{\sphinxupquote{BinaryLabel}}. A dataset with two columns: \sphinxcode{\sphinxupquote{example}} and \sphinxcode{\sphinxupquote{value}}. The column \sphinxcode{\sphinxupquote{value}} is a binary number (0 or 1).

\item {} 
  \sphinxcode{\sphinxupquote{MulticlassLabel}}. A dataset with the same structure as the previous one but where the column \sphinxcode{\sphinxupquote{value}} can take more than two values.

\item {} 
  \sphinxcode{\sphinxupquote{RealLabel}}. In this case, the column \sphinxcode{\sphinxupquote{value}} can take any numeric value.

\end{itemize}

The probabilistic types are used by some algorithms to provide more information about the confidence of each
class value for an specific example.
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{BinarySoftLabel}}. A dataset with two columns: \sphinxcode{\sphinxupquote{example}} and \sphinxcode{\sphinxupquote{prob}}. The column  \sphinxcode{\sphinxupquote{prob}}
represents the probability of the example being positive.

\item {} 
\sphinxcode{\sphinxupquote{MultiSoftLabel}}: A dataset with three columns: \sphinxcode{\sphinxupquote{example}}, \sphinxcode{\sphinxupquote{class}}, \sphinxcode{\sphinxupquote{prob}}. This last column 
represents the probability of the example taking the class in the column \sphinxcode{\sphinxupquote{class}}.

\end{itemize}


\section{Methods}
\label{\detokenize{package/design:methods}}
All the implemented methods are in the \sphinxcode{\sphinxupquote{methods}} subpackage and are mostly independent of each other. The MajorityVoting algorithms are the the only exception,
as most of the other methods use them in the initialization step. Apart from that, each algorithm 
is implemented in its specific file.  This makes it easier to extend the package with new algorithms. Although independent, all the algorithms have
a similar interface, which facilitates their use. To execute an algorithm, the user normally needs to use the \sphinxcode{\sphinxupquote{apply}} method of the model (which in \sphinxcode{\sphinxupquote{scala}}, is equivalent to applying the object itself), as shown below

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\PYG{k}{val} \PYG{n}{model} \PYG{k}{=} \PYG{n+nc}{IBCC}\PYG{o}{(}\PYG{n}{annotations}\PYG{o}{)}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

After the algorithm completes its execution, an object is returned, which has information about the ground truth estimations and
other estimations that are dependent on the chosen algorithm.

The only algorithm that does not follow this pattern is \sphinxcode{\sphinxupquote{MajorityVoting}}, which has methods for each of the class types and also to obtain
probabilistic labels. See the API Docs for details.


\chapter{Methods}
\label{\detokenize{package/methods:methods}}\label{\detokenize{package/methods:id1}}\label{\detokenize{package/methods::doc}}
You can find the methods implemented in the library below. All of them contain a link to its API where you
can find more information. Besides, in the table column, you can find a link to the reference. 


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Methods implemented in spark-crowd}\label{\detokenize{package/methods:id8}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Binary
&\sphinxstyletheadfamily 
Multiclass
&\sphinxstyletheadfamily 
Real
&\sphinxstyletheadfamily 
Reference
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.MajorityVoting\$}{MajorityVoting}
&
\(\surd\)
&
\(\surd\)
&
\(\surd\)
&\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.DawidSkene\$}{DawidSkene}
&
\(\surd\)
&
\(\surd\)
&&
\sphinxhref{https://www.jstor.org/stable/2346806?seq=1\#page\_scan\_tab\_contents}{JRSS}
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.IBCC\$}{IBCC}
&
\(\surd\)
&
\(\surd\)
&&
\sphinxhref{http://proceedings.mlr.press/v22/kim12.html}{AISTATS}
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.GLAD\$}{GLAD}
&
\(\surd\)
&&&
\sphinxhref{https://papers.nips.cc/paper/3644-whose-vote-should-count-more-optimal-integration-of-labels-from-labelers-of-unknown-expertise}{NIPS}
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.CGLAD\$}{CGLAD}
&
\(\surd\)
&&&
\sphinxhref{https://aida.ii.uam.es/ideal2018/\#!/main}{IDEAL}
\\
\hline
Raykar
&
\(\surd\)
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.RaykarBinary\$}{RaykarBinary}
&
\(\surd\)
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.RaykarBinary\$}{RaykarMulti}
&
\(\surd\)
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.RaykarBinary\$}{RaykarCont}
&
\sphinxhref{http://jmlr.csail.mit.edu/papers/v11/raykar10a.html}{JMLR}
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.CATD\$}{CATD}
&&&
\(\surd\)
&
\sphinxhref{http://www.vldb.org/pvldb/vol8/p425-li.pdf}{VLDB}
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.PM\$}{PM}
&&&
\(\surd\)
&
\sphinxhref{https://dl.acm.org/citation.cfm?id=2588555.2610509}{SIGMOD}
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.PMTI\$}{PMTI}
&&&
\(\surd\)
&
\sphinxhref{http://www.vldb.org/pvldb/vol10/p541-zheng.pdf}{VLDB2}
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Below, we provide a short summary of each method (see the references for the details). 


\section{MajorityVoting}
\label{\detokenize{package/methods:id2}}
With this, we refer to the mean for continuous target variables and the most frequent class (the mode) for the discrete
case. Expressing these methods in terms of annotator accuracy, they suppose that all the annotators have
the same experience. Therefore, their contributions are weighted equally. Apart from the classical mean and
most frequent class, we also provide methods that return the proportion of each class value for each example.
See the API Docs for more information.


\section{DawidSkene}
\label{\detokenize{package/methods:id3}}
This method estimates the accuracy of the annotators from the annotations themselves. For this, it uses the EM
algorithm, starting from the most frequent class and improving the estimations through several iterations. The
algorithm returns both the estimation of the ground truth and the accuracy of the annotators (a confusion
matrix for each). This algorithm is a good alternative when looking for a simple way of aggregating annotations
without the assumption that all of the annotators are equally accurate.


\section{IBCC}
\label{\detokenize{package/methods:id4}}
This method is similar to the previous one but uses probabilistic estimations for the classes. For each example,
the model returns probabilities for each class, so they can be useful in problems where a probability is needed.
It shows a good compromise between the complexity of the model and its performance, as can be seen in \sphinxhref{https://zhydhkcws.github.io/crowd\_truth\_inference/index.html}{here} 
or in our {\hyperref[\detokenize{package/other:comparison-with-other-packages}]{\sphinxcrossref{\DUrole{std,std-ref}{own experimentation}}}}.


\section{GLAD}
\label{\detokenize{package/methods:id5}}
This method estimates both the accuracy of the annotators (one parameter per annotator) and the difficulty
of each example (a parameter for each instance) through EM algorithm and gradient descent. This computational complexity
comes at a cost of a higher execution time in general. However, GLAD, and its enhancement, CGLAD, also implemented here, are the only two algorithms 
capable of estimating the difficulty of the instances.


\section{CGLAD}
\label{\detokenize{package/methods:id6}}
This method is an enhancement over the original GLAD algorithm to tackle bigger datasets more easily, using
clustering techniques over the examples to reduce the number of parameters to be estimated. It follows a similar
learning process to GLAD algorithm.


\section{Raykar’s algorithms}
\label{\detokenize{package/methods:raykar-s-algorithms}}
We implement the three methods proposed in the paper Learning from crowds 
(see \sphinxhref{http://jmlr.csail.mit.edu/papers/v11/raykar10a.html}{the paper} for the details) 
for learning
from crowdsourced data when features are available. These methods use an annotations dataset and the 
features for each instance. The algorithms infers together a logistic
model (or a regression model, for the continuous case), the ground truth
and the quality of the annotators, which are all returned by the methods in our package.


\section{CATD}
\label{\detokenize{package/methods:id7}}
This method estimates both the quality of the annotators (as a weight in the aggregation) and the ground truth
for continuous target variables. From the annotations, it estimates which annotators provide better labels.
Then, it assigns more weight to them for the aggregation. In the package, only
the continuous version is implemented as other algorithms seem to work better in the discrete cases (see \sphinxhref{https://zhydhkcws.github.io/crowd\_truth\_inference/index.html}{this paper} for more information).


\section{PM and PMTI}
\label{\detokenize{package/methods:pm-and-pmti}}
They are methods for continuous target variables. We implement two versions, one following the formulas appearing
in the original paper, and the modification implemented in  \sphinxhref{https://zhydhkcws.github.io/crowd\_truth\_inference/index.html}{this package}. The modification obtains better results in our experimentation 
(see {\hyperref[\detokenize{package/other:comparison}]{\sphinxcrossref{\DUrole{std,std-ref}{Comparison with other packages}}}}).


\chapter{Examples}
\label{\detokenize{usage/examples:examples}}\label{\detokenize{usage/examples::doc}}
In this page we provide examples for several of the algorithms in the library.
You can find the data used for the examples in the Github repository.


\section{MajorityVoting}
\label{\detokenize{usage/examples:majorityvoting}}
The example below shows how to use the MajorityVoting algorithm for estimating the ground truth for a binary target variable. 

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.methods.MajorityVoting}
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.types.BinaryAnnotation}

\PYG{k}{val} \PYG{n}{exampleFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}data/binary\PYGZhy{}ann.parquet\PYGZdq{}}

\PYG{k}{val} \PYG{n}{exampleDataBinary} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{exampleFile}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{BinaryAnnotation}\PYG{o}{]}

\PYG{k}{val} \PYG{n}{muBinary} \PYG{k}{=} \PYG{n+nc}{MajorityVoting}\PYG{o}{.}\PYG{n}{transformBinary}\PYG{o}{(}\PYG{n}{exampleDataBinary}\PYG{o}{)}

\PYG{n}{muBinary}\PYG{o}{.}\PYG{n}{show}\PYG{o}{(}\PYG{o}{)}
\end{sphinxVerbatim}

The method returns a result similar to this one:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+}
\PYG{o}{\textbar{}}\PYG{n}{example}\PYG{o}{\textbar{}}\PYG{n}{value}\PYG{o}{\textbar{}}
\PYG{o}{+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+}
\PYG{o}{\textbar{}}     \PYG{l+m+mi}{26}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{0}\PYG{o}{\textbar{}}
\PYG{o}{\textbar{}}     \PYG{l+m+mi}{29}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{1}\PYG{o}{\textbar{}}
\PYG{o}{\textbar{}}    \PYG{l+m+mi}{474}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{0}\PYG{o}{\textbar{}}
\PYG{o}{\textbar{}}    \PYG{l+m+mi}{964}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{1}\PYG{o}{\textbar{}}
\PYG{o}{\textbar{}}     \PYG{l+m+mi}{65}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{0}\PYG{o}{\textbar{}}
\PYG{o}{\textbar{}}    \PYG{l+m+mi}{191}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{0}\PYG{o}{\textbar{}}
\PYG{o}{\textbar{}}    \PYG{l+m+mi}{418}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{1}\PYG{o}{\textbar{}}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

MajorityVoting algorithms assume that all the annotators are equally accurate, so they choose the
most frequent annotation as the ground truth label. Because of this, they only return the ground
truth for the problem.

The data file in this example follow the format from the \sphinxcode{\sphinxupquote{BinaryAnnotation}} type:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{example}\PYG{o}{,} \PYG{n}{annotator}\PYG{o}{,} \PYG{n}{value}
      \PYG{l+m+mi}{0}\PYG{o}{,}         \PYG{l+m+mi}{0}\PYG{o}{,}     \PYG{l+m+mi}{1}
      \PYG{l+m+mi}{0}\PYG{o}{,}         \PYG{l+m+mi}{1}\PYG{o}{,}     \PYG{l+m+mi}{0}
      \PYG{l+m+mi}{0}\PYG{o}{,}         \PYG{l+m+mi}{2}\PYG{o}{,}     \PYG{l+m+mi}{1}
      \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

In this example, we use a \sphinxcode{\sphinxupquote{.parquet}} data file, which is usually a good option in terms of
efficiency. However, we do not limit the types of files you can use, as long as they can be
converted to typed datasets of \sphinxcode{\sphinxupquote{BinaryAnnotation}}, \sphinxcode{\sphinxupquote{MulticlassAnnotation}} or \sphinxcode{\sphinxupquote{RealAnnotation}}.
The algorithms do assume that there are no missing examples or annotators.

Specifically, MajorityVoting can make predictions both for discrete classes (\sphinxcode{\sphinxupquote{BinaryAnnotation}} and
\sphinxcode{\sphinxupquote{MulticlassAnnotation}}) and continuous-valued target variables (\sphinxcode{\sphinxupquote{RealAnnotation}}). You can find
information about these methods in the API Docs.


\section{DawidSkene}
\label{\detokenize{usage/examples:dawidskene}}
This algorithm is one of the most recommended both for its simplicity and its good results generally. 

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.methods.DawidSkene}
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.types.MulticlassAnnotation}

\PYG{k}{val} \PYG{n}{exampleFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}examples/data/multi\PYGZhy{}ann.parquet\PYGZdq{}}

\PYG{k}{val} \PYG{n}{exampleData} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{exampleFile}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{MulticlassAnnotation}\PYG{o}{]}

\PYG{k}{val} \PYG{n}{mode} \PYG{k}{=} \PYG{n+nc}{DawidSkene}\PYG{o}{(}\PYG{n}{exampleData}\PYG{o}{,} \PYG{n}{eMIters}\PYG{k}{=}\PYG{l+m+mi}{10}\PYG{o}{,} \PYG{n}{emThreshold}\PYG{k}{=}\PYG{l+m+mf}{0.001}\PYG{o}{)}

\PYG{k}{val} \PYG{n}{pred} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getMu}\PYG{o}{(}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{MulticlassLabel}\PYG{o}{]}

\PYG{k}{val} \PYG{n}{annprec} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getAnnotatorPrecision}\PYG{o}{(}\PYG{o}{)}
\end{sphinxVerbatim}

In the implementation, two parameters are used for controlling the algorithm execution, the maximum number
of EM iterations and the threshold for the likelihood change. The execution stops if the number of iterations reaches 
the established maximum or if the change in likelihood is less than the threshold. You do not need to
provide these parameters, as they have default values.

Once executed, the model provide an estimation of the ground truth
and an estimation of the quality of each annotator, in the form of a confusion matrix. This information can be obtained 
as shown on the example.


\section{GLAD}
\label{\detokenize{usage/examples:glad}}
The GLAD algorithm is interesting as it provides both annotator accuracies and example difficulties obtained
solely from the annotations. An example of how to use it can be found below.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.methods.Glad}
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.types.BinaryAnnotation}

\PYG{k}{val} \PYG{n}{annFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}data/binary\PYGZhy{}ann.parquet\PYGZdq{}}

\PYG{k}{val} \PYG{n}{annData} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{annFile}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{BinaryAnnotation}\PYG{o}{]}

\PYG{k}{val} \PYG{n}{mode} \PYG{k}{=} \PYG{n+nc}{Glad}\PYG{o}{(}\PYG{n}{annData}\PYG{o}{,}
                  \PYG{n}{eMIters}\PYG{k}{=}\PYG{l+m+mi}{5}\PYG{o}{,} \PYG{c+c1}{//Maximum number of iterations of EM algorithm}
                  \PYG{n}{eMThreshold}\PYG{k}{=}\PYG{l+m+mf}{0.1}\PYG{o}{,} \PYG{c+c1}{//Threshold for likelihood changes}
                  \PYG{n}{gradIters}\PYG{k}{=}\PYG{l+m+mi}{30}\PYG{o}{,} \PYG{c+c1}{//Gradient descent max number of iterations}
                  \PYG{n}{gradTreshold}\PYG{k}{=}\PYG{l+m+mf}{0.5}\PYG{o}{,} \PYG{c+c1}{//Gradient descent threshold}
                  \PYG{n}{gradLearningRate}\PYG{k}{=}\PYG{l+m+mf}{0.01}\PYG{o}{,} \PYG{c+c1}{//Gradient descent learning rate}
                  \PYG{n}{alphaPrior}\PYG{k}{=}\PYG{l+m+mi}{1}\PYG{o}{,} \PYG{c+c1}{//Alpha first value (GLAD specific)}
                  \PYG{n}{betaPrior}\PYG{k}{=}\PYG{l+m+mi}{1}\PYG{o}{)} \PYG{c+c1}{//Beta first value (GLAD specific)}

\PYG{k}{val} \PYG{n}{pred} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getMu}\PYG{o}{(}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{BinarySoftLabel}\PYG{o}{]}

\PYG{k}{val} \PYG{n}{annprec} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getAnnotatorPrecision}\PYG{o}{(}\PYG{o}{)}

\PYG{k}{val} \PYG{n}{annprec} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getInstanceDifficulty}\PYG{o}{(}\PYG{o}{)}
\end{sphinxVerbatim}

This model as implemented in the library is only compatible with binary class problems. It has a
higher number of free parameters in comparison with the previous algorithms, but we provided default
values for all of them for convenience. The meaning of each of these parameters is commented in the
example above, as it is in the API Docs. The annotator precision is given as a vector, with an
entry for each annotator. The difficulty is given in the form of a DataFrame, returning
a difficulty value for each example. For more information, you can consult the documentation
and/or the paper.


\section{RaykarBinary, RaykarMulti and RaykarCont}
\label{\detokenize{usage/examples:raykarbinary-raykarmulti-and-raykarcont}}
We implement the three variants of this algorithm, two for discrete target variables (\texttt{RaykarBinary} and \texttt{RaykarMulti}) 
and one for continuous variables (\texttt{RaykarCont}).
These algorithms have in common that they are able to use features to estimate the ground truth
and even learn a linear model. The model is also able to use prior information about annotators,
which can be useful to add more confidence to certain annotators. In the next example we show
how to use this model adding a prior which indicates that we trust a lot in the first annotator
and that we know that the second annotator is not reliable.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.methods.RaykarBinary}
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.types.BinaryAnnotation}

\PYG{k}{val} \PYG{n}{exampleFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}data/binary\PYGZhy{}data.parquet\PYGZdq{}}
\PYG{k}{val} \PYG{n}{annFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}data/binary\PYGZhy{}ann.parquet\PYGZdq{}}

\PYG{k}{val} \PYG{n}{exampleData} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{exampleFile}\PYG{o}{)}
\PYG{k}{val} \PYG{n}{annData} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{annFile}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{BinaryAnnotation}\PYG{o}{]}

\PYG{c+c1}{//Preparing priors}
\PYG{k}{val} \PYG{n}{nAnn} \PYG{k}{=} \PYG{n}{annData}\PYG{o}{.}\PYG{n}{map}\PYG{o}{(}\PYG{k}{\PYGZus{}}\PYG{o}{.}\PYG{n}{annotator}\PYG{o}{)}\PYG{o}{.}\PYG{n}{distinct}\PYG{o}{.}\PYG{n}{count}\PYG{o}{(}\PYG{o}{)}\PYG{o}{.}\PYG{n}{toInt}

\PYG{k}{val} \PYG{n}{a} \PYG{k}{=} \PYG{n+nc}{Array}\PYG{o}{.}\PYG{n}{fill}\PYG{o}{[}\PYG{k+kt}{Double}\PYG{o}{]}\PYG{o}{(}\PYG{n}{nAnn}\PYG{o}{,}\PYG{l+m+mi}{2}\PYG{o}{)}\PYG{o}{(}\PYG{l+m+mf}{2.0}\PYG{o}{)} \PYG{c+c1}{//Uniform prior}
\PYG{k}{val} \PYG{n}{b} \PYG{k}{=} \PYG{n+nc}{Array}\PYG{o}{.}\PYG{n}{fill}\PYG{o}{[}\PYG{k+kt}{Double}\PYG{o}{]}\PYG{o}{(}\PYG{n}{nAnn}\PYG{o}{,}\PYG{l+m+mi}{2}\PYG{o}{)}\PYG{o}{(}\PYG{l+m+mf}{2.0}\PYG{o}{)} \PYG{c+c1}{//Uniform prior}

\PYG{c+c1}{//Give first annotator more confidence}
\PYG{n}{a}\PYG{o}{(}\PYG{l+m+mi}{0}\PYG{o}{)}\PYG{o}{(}\PYG{l+m+mi}{0}\PYG{o}{)} \PYG{o}{+=} \PYG{l+m+mi}{1000}
\PYG{n}{b}\PYG{o}{(}\PYG{l+m+mi}{0}\PYG{o}{)}\PYG{o}{(}\PYG{l+m+mi}{0}\PYG{o}{)} \PYG{o}{+=} \PYG{l+m+mi}{1000}

\PYG{c+c1}{//Give second annotator less confidence}
\PYG{c+c1}{//Annotator 1}
\PYG{n}{a}\PYG{o}{(}\PYG{l+m+mi}{1}\PYG{o}{)}\PYG{o}{(}\PYG{l+m+mi}{1}\PYG{o}{)} \PYG{o}{+=} \PYG{l+m+mi}{1000}
\PYG{n}{b}\PYG{o}{(}\PYG{l+m+mi}{1}\PYG{o}{)}\PYG{o}{(}\PYG{l+m+mi}{1}\PYG{o}{)} \PYG{o}{+=} \PYG{l+m+mi}{1000}


\PYG{c+c1}{//Applying the learning algorithm}
\PYG{k}{val} \PYG{n}{mode} \PYG{k}{=} \PYG{n+nc}{RaykarBinary}\PYG{o}{(}\PYG{n}{exampleData}\PYG{o}{,} \PYG{n}{annData}\PYG{o}{,}
                          \PYG{n}{eMIters}\PYG{k}{=}\PYG{l+m+mi}{5}\PYG{o}{,}
                          \PYG{n}{eMThreshold}\PYG{k}{=}\PYG{l+m+mf}{0.001}\PYG{o}{,}
                          \PYG{n}{gradIters}\PYG{k}{=}\PYG{l+m+mi}{100}\PYG{o}{,}
                          \PYG{n}{gradThreshold}\PYG{k}{=}\PYG{l+m+mf}{0.1}\PYG{o}{,}
                          \PYG{n}{gradLearning}\PYG{k}{=}\PYG{l+m+mf}{0.1}
                          \PYG{n}{a\PYGZus{}prior}\PYG{k}{=}\PYG{n+nc}{Some}\PYG{o}{(}\PYG{n}{a}\PYG{o}{)}\PYG{o}{,} \PYG{n}{b\PYGZus{}prior}\PYG{k}{=}\PYG{n+nc}{Some}\PYG{o}{(}\PYG{n}{b}\PYG{o}{)}\PYG{o}{)}

\PYG{c+c1}{//Get MulticlassLabel with the class predictions}
\PYG{k}{val} \PYG{n}{pred} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getMu}\PYG{o}{(}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{BinarySoftLabel}\PYG{o}{]}

\PYG{c+c1}{//Annotator precision matrices}
\PYG{k}{val} \PYG{n}{annprec} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getAnnotatorPrecision}\PYG{o}{(}\PYG{o}{)}
\end{sphinxVerbatim}

Apart from the feature matrix and the priors, the meaning of the parameters is the same as in the previous examples.
The priors are matrices of dimension $A \times 2$ where $A$ is the number of annotators. In each row we have the hyperparameters of a Beta distribution for each annotator.
The \sphinxcode{\sphinxupquote{a\_prior}} gives prior information about the ability of annotators to correctly classify a positive example. The
\sphinxcode{\sphinxupquote{b\_prior}} does the same thing but for the negative examples. More information about this method as well as the methods
for discrete and continuous target variables can be found in the API docs.


\section{CATD}
\label{\detokenize{usage/examples:catd}}
This method allows to estimate continuous-value target variables from annotations.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.methods.CATD}
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.types.RealAnnotation}

\PYG{n}{sc}\PYG{o}{.}\PYG{n}{setCheckpointDir}\PYG{o}{(}\PYG{l+s}{\PYGZdq{}checkpoint\PYGZdq{}}\PYG{o}{)}

\PYG{k}{val} \PYG{n}{annFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}examples/data/cont\PYGZhy{}ann.parquet\PYGZdq{}}

\PYG{k}{val} \PYG{n}{annData} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{annFile}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{RealAnnotation}\PYG{o}{]}

\PYG{c+c1}{//Applying the learning algorithm}
\PYG{k}{val} \PYG{n}{mode} \PYG{k}{=} \PYG{n+nc}{CATD}\PYG{o}{(}\PYG{n}{annData}\PYG{o}{,} \PYG{n}{iterations}\PYG{k}{=}\PYG{l+m+mi}{5}\PYG{o}{,}
                          \PYG{n}{threshold}\PYG{k}{=}\PYG{l+m+mf}{0.1}\PYG{o}{,}
                          \PYG{n}{alpha}\PYG{k}{=}\PYG{l+m+mf}{0.05}\PYG{o}{)}

\PYG{c+c1}{//Get MulticlassLabel with the class predictions}
\PYG{k}{val} \PYG{n}{pred} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{mu}

\PYG{c+c1}{//Annotator precision matrices}
\PYG{k}{val} \PYG{n}{annprec} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{weights}
\end{sphinxVerbatim}

It returns a model from which you can get the ground truth estimation and
also the annotator weight used (more weight means a better annotator).
The algorithm uses parameters such as \sphinxcode{\sphinxupquote{iterations}} and \sphinxcode{\sphinxupquote{threshold}} for
controlling the execution, and also \sphinxcode{\sphinxupquote{alpha}}, which is a parameter of the model
(check the API docs for more information).


\chapter{Comparison with other packages}
\label{\detokenize{package/other:comparison-with-other-packages}}\label{\detokenize{package/other:comparison}}\label{\detokenize{package/other::doc}}
There exist other packages implementing similar methods in other languages, but with
different aims in mind. To our knowledge, there are 2 software packages with the goal
of learning from crowdsourced data:
\begin{itemize}
\item {} 
\sphinxhref{http://ceka.sourceforge.net/}{Ceka}: it is a Java software package based on WEKA, with
a great number of methods that can be used to learn from crowdsource data.

\item {} 
\sphinxhref{https://zhydhkcws.github.io/crowd\_truth\_inference/index.html/}{Truth inference in Crowdsourcing} makes available a collection
of methods in Python to learn from crowdsourced data.

\end{itemize}

Both are useful packages when dealing with crowdsourced data, with a focus mainly on research. Differently, \sphinxtitleref{spark-crowd} is 
useful not only in research, but also in production. It provides a clear usage interface as well as software tests for all of its methods with a high test 
coverage. Moreover, methods have been implemented with a focus on scalability, so it is useful in a wide variety of situations. A
comparison of the methods over a set of datasets is provided in this section, taking into account both quality of the models and execution time.


\section{Data}
\label{\detokenize{package/other:data}}
For this performance test we use simulated datasets of increasing size and a real multiclass dataset:
\begin{itemize}
\item {} 
\sphinxstylestrong{binary1-4}: simulated binary class datasets with 10K, 100K, 1M and 10M instances respectively. Each of them
has 10 simulated annotations per instance, and the ground truth for each example is known (but not used in the
learning process). The accuracy shown in the tables is obtained over this known ground truth.

\item {} 
\sphinxstylestrong{cont1-4}: simulated continuous target variable datasets, with 10k, 100k, 1M and 10M instances respectively. Each of them
has 10 simulated annotations per instance, and the ground truth for each example is known (but not used in the
learning process). The Mean Absolute Error is obtained over this known ground truth.

\item {} 
\sphinxstylestrong{crowdscale}. A real multiclass dataset from the \sphinxstyleemphasis{Crowdsourcing at Scale} challenge. The data is comprised of 98979 instances,
evaluated by, at least, 5 annotators, for a total of 569375 answers. We only have ground truth for the 0.3\% of the data,
which is used for evaluation.

\end{itemize}

All the datasets are available through this \sphinxhref{https://www.dropbox.com/sh/odmhdf83latvezu/AAB6om3Oy7-waf-msIvk9yX6a?dl=0}{link}


\section{CEKA}
\label{\detokenize{package/other:id1}}
To compare our methods with Ceka, we used two of the main methods implemented in both packages, MajorityVoting and DawidSkene. Ceka and
spark-crowd also implement GLAD and Raykar’s algorithms. However, in Ceka, these algorithms are implemented using wrappers to other libraries.
The library for the GLAD algorithm is not available on our platform, as it is given as an EXE Windows file, and the wrapper for Raykar’s algorithms
does not admit any configuration parameters.

We provide the results of the execution in terms of accuracy (Acc) and time (in seconds). For our package, we also include
the execution time for a cluster (tc) with 3 executor nodes of 10 cores and 30Gb of memory each.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Comparison with Ceka}\label{\detokenize{package/other:id3}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily MajorityVoting
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily DawidSkene
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Ceka
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Ceka
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
binary1
&
0.931
&
21
&
0.931
&
11
&
7
&
0.994
&
57
&
0.994
&
31
&
32
\\
\hline
binary2
&
0.936
&
15983
&
0.936
&
11
&
7
&
0.994
&
49259
&
0.994
&
60
&
51
\\
\hline
binary3
&
X
&
X
&
0.936
&
21
&
8
&
X
&
X
&
0.994
&
111
&
69
\\
\hline
binary4
&
X
&
X
&
0.936
&
57
&
37
&
X
&
X
&
0.994
&&\\
\hline
crowdscale
&
0.88
&
10458
&
0.9
&
13
&
7
&
0.89
&
30999
&
0.9033
&
447
&
86
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Regarding accuracy, both packages achieve comparable results. However, regarding execution time, spark-crowd obtains
significantly better results among all datasets, especially on the bigger ones, where it tackle problems that
Ceka is not able to solve. 

%You can see the speedup results in the table below.
%
%\begin{savenotes}\sphinxattablestart
%\centering
%\sphinxcapstartof{table}
%\sphinxcaption{Speedup in comparison to Ceka}\label{\detokenize{package/other:id4}}
%\sphinxaftercaption
%\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
%\hline
%\sphinxstyletheadfamily &\sphinxstartmulticolumn{4}%
%\begin{varwidth}[t]{\sphinxcolwidth{4}{5}}
%\sphinxstyletheadfamily MajorityVoting  \textbar{}      DawidSkene
%\par
%\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
%\sphinxstopmulticolumn
%\\
%\hline\sphinxstyletheadfamily 
%Method
%&\sphinxstyletheadfamily 
%t1
%&\sphinxstyletheadfamily 
%tc
%&\sphinxstyletheadfamily 
%t1
%&\sphinxstyletheadfamily 
%tc
%\\
%\hline
%binary1
%&
%1.86
%&
%2.93
%&
%1.84
%&
%1.78
%\\
%\hline
%binary2
%&
%1453
%&
%2283
%&
%272
%&
%1146
%\\
%\hline
%crowdscale
%&
%804
%&
%1494
%&
%69
%&
%360
%\\
%\hline
%\end{tabulary}
%\par
%\sphinxattableend\end{savenotes}
%
%We can see that spark-crowd obtains a high speedup in bigger datasets and performs
%slightly better in the smaller ones.


\section{Truth inference in crowdsourcing}
\label{\detokenize{package/other:id2}}
Now we compare \texttt{spark-crowd} with the methods implemented by the authors. Although they
can certainly be used to compare and test algorithms, the integration of these
methods into a large ecosystem might be difficult, as the authors do not provide
a software package structure. Nevertheless, as it is an available package with a great number
of methods, a comparison with them is advisable. 

For the experimentation, the same datasets are used as well as the same environments. 
In this case, a higher number of models can be compared, as most of the methods are written in python. 
However, the methods can only be applied to binary or continuous target variables. As far as we
know, the use of multiclass target variables is not possible as it is the case of feature information for
Raykar's methods. 

First, we compare the algorithms capable of learning from binary classes.
In this category, MajorityVoting, DawidSkene, GLAD and IBCC are compared. For each dataset, the  
results in terms of Accuracy (Acc) and time (in seconds) are obtained. The table below shows the results for
MajorityVoting and DawidSkene. Both packages obtain the same results in terms of
accuracy. For the smallest datasets, the overhead imposed by parallelism makes Truth-Inf a better choice,
at least in terms of execution time. However, as the size of the datasets increase, and especially, in the last two
cases, the speedup obtained by our algorithm is notable. In the case of DawidSkene, the Truth-inf
package is not able to complete the execution because of memory constraints in the largest dataset.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{MajorityVoting and DawidSkene comparison}\label{\detokenize{package/other:id5}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily MajorityVoting
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily DawidSkene
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
binary1
&
0.931
&
1
&
0.931
&
11
&
7
&
0.994
&
12
&
0.994
&
31
&
32
\\
\hline
binary2
&
0.936
&
8
&
0.936
&
11
&
7
&
0.994
&
161
&
0.994
&
60
&
51
\\
\hline
binary3
&
0.936
&
112
&
0.936
&
21
&
8
&
0.994
&
1705
&
0.994
&
111
&
69
\\
\hline
binary4
&
0.936
&
2908
&
0.936
&
57
&
37
&
M
&
M
&
0.994
&
703
&
426
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Next we show the results for GLAD and IBCC. As can be seen, both packages obtain similar results
in terms of accuracy. Regarding execution time, they obtain comparable results
in the two smaller datasets (with a slight speedup in \sphinxcode{\sphinxupquote{binary2}}) for the GLAD algorithm. However, for
this algorithm, Truth-inf is not able to complete the execution in the two largest datasets.
In the case of IBCC, the speedup obtained by our library starts to be noticeable from the second dataset on. It is also noticeable that 
\texttt{Truth-Inf} did not complete the execution for the last dataset. 


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{GLAD and IBCC comparison}\label{\detokenize{package/other:id6}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily GLAD
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily IBCC
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
binary1
&
0.994
&
1185
&
0.994
&
1568
&
1547
&
0.994
&
22
&
0.994
&
74
&
67
\\
\hline
binary2
&
0.994
&
4168
&
0.994
&
2959
&
2051
&
0.994
&
372
&
0.994
&
97
&
76
\\
\hline
binary3
&
X
&
X
&
0.491
&
600
&
226
&
0.994
&
25764
&
0.994
&
203
&
129
\\
\hline
binary4
&
X
&
X
&
0.974
&
2407
&
1158
&
X
&
X
&
X
&
1529
&
823
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Note that the performance of GLAD algorithm seems to degrade in the bigger datasets. 
This may be due to the ammount of parameters the algorithm needs to estimate. 
A way to improve the estimation goes through decreasing the learning rate, which
makes the algorithm slower, as it needs many more iterations to obtain a good solution. 
This makes the algorithm
unsuitable for several big data contexts.  To tackle this kind of problems, we developed an enhancement, CGLAD,
which is included in this package (see the last section of this page for results of other
methods in the package, as well as this enhancement).
Next, we analize methods that are able to learn from continuous target variables: MajorityVoting (mean), CATD and PM (with mean initialization). We show the results in terms of MAE (mean absolute error) and time (in seconds). The
results for MajorityVoting and CATD can be found in the table below.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
  \sphinxcaption{MajorityVoting (mean) and CATD comparison}\label{\detokenize{package/other:id7}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily MajorityVoting (mean)
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily CATD
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
cont1
&
1.234
&
1
&
1.234
&
6
&
8
&
0.324
&
207
&
0.324
&
25
&
28
\\
\hline
cont2
&
1.231
&
8
&
1.231
&
7
&
9
&
0.321
&
10429
&
0.321
&
26
&
24
\\
\hline
cont3
&
1.231
&
74
&
1.231
&
12
&
13
&
X
&
X
&
0.322
&
42
&
38
\\
\hline
cont4
&
1.231
&
581
&
1.231
&
56
&
23
&
X
&
X
&
0.322
&
247
&
176
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

As can be seen in the table, both packages obtain similar results regarding MAE. Regarding execution time,
the implementation of MajorityVoting from the \texttt{Truth-inf} package obtains good results, especially in the smaller dataset. 
It is worth pointing out that, for the smallest datasets, the increase overhead imposed by parallelism makes the execution time of 
our package a little worse in comparison.
However, as datasets increase in size, the speedup obtained by our package is notable, even in MajorityVoting,
which is less complex computationally. Regarding CATD, Truth-inf seems not to be able to solve the two bigger problems
in a reasonable time. However, they can be solved by our package in a small ammount of time. Even for the smallest sizes, 
our package obtains a high speedup in comparison to \texttt{Truth-inf} for CATD.

In the table below you can find the results for PM and PMTI algorithms.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{PM and PMTI comparison}\label{\detokenize{package/other:id8}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily PM
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily PMTI
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
cont1
&
0.495
&
77
&
0.495
&
57
&
51
&
0.388
&
139
&
0.388
&
68
&
61
\\
\hline
cont2
&
0.493
&
8079
&
0.495
&
76
&
57
&
0.386
&
14167
&
0.386
&
74
&
58
\\
\hline
cont3
&
X
&
X
&
0.494
&
130
&
97
&
X
&
X
&
0.387
&
143
&
98
\\
\hline
cont4
&
X
&
X
&
0.494
&
769
&
421
&
X
&
X
&
0.387
&
996
&
475
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Although similar, the modification implemented in Truth-inf from the original algorithm seems to be more
accurate. 
Even in the smallest sizes, our package obtains a slight speedup. However, as the datasets increase in size,
our package is able to obtain a much higher speedup. 

\section{Other methods}
\label{\detokenize{package/other:other-methods}}
To complete our experimentation, next we focus on other methods other methods implemented by our package that
are not implemented by \texttt{Ceka} or \texttt{Truth-Inf}. These methods are the full implementation of the Raykar’s
algorithms (taking into account the features of the instances) and the enhancement over the GLAD algorithm. As a
note, Truth-inf implements a version of Raykar’s algorithms that does not use the features of the instances. First,
we show the results obtained by the Raykar’s methods for discrete target variables.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Raykar’s methods in spark-crowd for discrete targets .}\label{\detokenize{package/other:id9}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{7}}
\sphinxstyletheadfamily RaykarBinary
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{7}}
\sphinxstyletheadfamily RaykarMulti
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{7}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{7}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
binary1
&
0.994
&
65
&
63
&
0.994
&
167
&
147
\\
\hline
binary2
&
0.994
&
92
&
74
&
0.994
&
241
&
176
\\
\hline
binary3
&
0.994
&
181
&
190
&
0.994
&
532
&
339
\\
\hline
binary4
&
0.994
&
1149
&
560
&
0.994
&
4860
&
1196
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Next we show the Raykar method for tackling continous target variables.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Raykar method for continuous target variables.}\label{\detokenize{package/other:id10}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{4}%
\begin{varwidth}[t]{\sphinxcolwidth{4}{5}}
\sphinxstyletheadfamily RaykarCont
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{4}%
\begin{varwidth}[t]{\sphinxcolwidth{4}{5}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{5}}
\sphinxstyletheadfamily t1
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstyletheadfamily 
tc
\\
\hline
cont1
&
0.994
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{5}}
31
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&
32
\\
\hline
cont2
&
0.994
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{5}}
60
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&
51
\\
\hline
cont3
&
0.994
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{5}}
111
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&
69
\\
\hline
cont4
&
0.994
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{5}}
703
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&
426
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Finally, we show the results for the CGLAD algorithm. As you can see, it obtains similar results to the GLAD algorithm
in the smallest instances but it performs much better in the larger ones. Regarding execution time, CGLAD obtains a high
speedup in the cases where accuracy results for both algorithms are similar. 


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{CGLAD, an enhancement over the GLAD algorithm.}\label{\detokenize{package/other:id11}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{4}}
\sphinxstyletheadfamily CGLAD
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{4}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
binary1
&
0.994
&
128
&
128
\\
\hline
binary2
&
0.995
&
233
&
185
\\
\hline
binary3
&
0.995
&
1429
&
607
\\
\hline
binary4
&
0.995
&
17337
&
6190
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\chapter{Contributors}
\label{\detokenize{package/contributors:contributors}}\label{\detokenize{package/contributors::doc}}
We are open to contributions in the form of bugs reports, enhancements or even new algorithms.


\section{Bug reports}
\label{\detokenize{package/contributors:bug-reports}}
Bugs are tracked using Github issues. When creating a bug report, please provide as much information as possible to help maintainers
reproduce the problem
\begin{itemize}
\item {} 
Use a clear and descriptive title for the issue to identify the problem.

\item {} 
Describe the exact steps which reproduce the problem in as many details as possible. For example, start by explaining how you prepared the data as well as how the package was installed and what version of the package are you using. When listing steps, do not just say what you did, but also explain how you did it.

\item {} 
Provide specific examples to illustrate the steps. Include links to files or GitHub projects. If you are providing snippets in the issue, use Markdown code blocks.

\item {} 
Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior.
Explain which behavior you expected to see instead and why.

\item {} 
If the problem is related to performance or memory, include a CPU profile capture with your report.

\end{itemize}

Provide more context by answering these questions:
\begin{itemize}
\item {} 
Did the problem start happening recently (e.g. after updating the version dependencies) or was this always a problem?

\item {} 
If the problem started happening recently, can you reproduce the problem in an older version? What is the most recent version in which the problem does not happen?

\item {} 
Can you reliably reproduce the issue? If not, provide details about how often the problem happens and under which conditions it normally happens.

\end{itemize}

Include details about your configuration and environment:
\begin{itemize}
\item {} 
Which version of spark-crowd are you using?

\item {} 
What is the name and version of the OS you are using?

\item {} 
Are you running the package in a virtual machine? If so, which VM software are you using and which operating systems and versions are used for the host and the guest?

\end{itemize}


\section{Suggesting enhancements}
\label{\detokenize{package/contributors:suggesting-enhancements}}
We are open to suggestions of new features and improvements to existing functionalities. Please, follow the guidelines to help maintainers and the community
understand your suggestions. When requesting an enhancement, please include as many details as possible.

Enhancement suggestions are tracked using Github Issues. To request an enhancement, create an issue and provide the following information:
\begin{itemize}
\item {} 
Use a clear and descriptive title for the issue to identify the suggestion.

\item {} 
Provide a step-by-step description of the suggested enhancement.

\item {} 
Provide specific examples to illustrate the steps. Include copy/pasteable snippets which you use in those examples, as Markdown code blocks.

\item {} 
Describe the current behavior and explain which behavior you expected to see instead and why.

\item {} 
Explain why this enhancement would be useful to the users.

\item {} 
Specify which version of the package you are using. Specify the name and version of the OS you are using.

\end{itemize}


\section{New algorithms}
\label{\detokenize{package/contributors:new-algorithms}}
We are also grateful for contributions of new algorithms, as long as they improve the results or add new functionalities to the ones existing in the package.
New algorithms must be published in peer-review publications to be considered. New algorithms must adhere to the architecture of this package and
take into account the scalability of the learning process.

To contribute with an algorithm, first create a request using Github Issues, for the maintainers to review the suggestion. This request should provide the following information:
\begin{itemize}
\item {} 
Publication where the algorithm details can be reviewed.

\item {} 
Explain why this algorithm would be useful to the users.

\end{itemize}

If the request is accepted, create a Github pull request with the new algorithm, as well as all the necessary types to use it, so that the maintainers can review the
code and add it to the package.


\renewcommand{\indexname}{Index}
\printindex
\end{document}
