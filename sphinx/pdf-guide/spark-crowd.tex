%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}





\title{spark-crowd Documentation}
\date{Nov 06, 2018}
\release{0.1.6}
\author{Enrique G. Rodrigo}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\maketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


Learning from crowdsourced Big Data


\bigskip\hrule\bigskip



\chapter{Quick Start}
\label{\detokenize{usage/quickstart:quick-start}}\label{\detokenize{usage/quickstart:quickstart}}\label{\detokenize{usage/quickstart::doc}}
You can start using our package easily through our \sphinxhref{https://www.docker.com/}{docker} image or through \sphinxhref{https://spark-packages.org/}{spark-packages}.
See {\hyperref[\detokenize{usage/installation:installation}]{\sphinxcrossref{\DUrole{std,std-ref}{Installation}}}}, for all installation alternatives (such as how to add the package as a dependency in your project).


\section{Start with our docker image}
\label{\detokenize{usage/quickstart:start-with-our-docker-image}}
The quickest way to try our package is using the
\sphinxhref{https://hub.docker.com/r/enriquegrodrigo/spark-crowd/}{provided docker image} with the latest version of
our package, as you won’t need to install anything (apart from docker).

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker pull enriquegrodrigo/spark\PYGZhy{}crowd
\end{sphinxVerbatim}

With it you can run the examples provided along with the
\sphinxhref{https://github.com/enriquegrodrigo/spark-crowd}{package}. For example,
to run \sphinxtitleref{DawidSkeneExample.scala} we can use:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker run \PYGZhy{}\PYGZhy{}rm \PYGZhy{}it \PYGZhy{}v \PYG{k}{\PYGZdl{}(}\PYG{n+nb}{pwd}\PYG{k}{)}/:/home/work/project enriquegrodrigo/spark\PYGZhy{}crowd DawidSkeneExample.scala
\end{sphinxVerbatim}

You can also open a spark shell with the library preloaded.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker run \PYGZhy{}\PYGZhy{}rm \PYGZhy{}it \PYGZhy{}v \PYG{k}{\PYGZdl{}(}\PYG{n+nb}{pwd}\PYG{k}{)}/:/home/work/project enriquegrodrigo/spark\PYGZhy{}crowd
\end{sphinxVerbatim}

So you can test your code directly. In this way you will not benefit from the advantages of Apache Spark
but you could use the algorithms with medium datasets (as docker can use several cores in your machine).


\section{Start with \sphinxtitleref{spark-packages}}
\label{\detokenize{usage/quickstart:start-with-spark-packages}}
If you have an installation of \sphinxhref{https://spark.apache.org/}{Apache Spark}  you can open a \sphinxtitleref{spark-shell} with
our package pre-loaded using:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
spark\PYGZhy{}shell \PYGZhy{}\PYGZhy{}packages com.enriquegrodrigo:spark\PYGZhy{}crowd\PYGZus{}2.11:0.2.0
\end{sphinxVerbatim}

Likewise, you can submit an application to your cluster that uses \sphinxtitleref{spark-crowd} using:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
spark\PYGZhy{}submit \PYGZhy{}\PYGZhy{}packages com.enriquegrodrigo:spark\PYGZhy{}crowd\PYGZus{}2.11:0.2.0 application.scala
\end{sphinxVerbatim}

To use this option you do not need to have a cluster of computers, you can also execute the code form
your local machine as Apache Spark can also be installed locally. For more information on how to install
Apache Spark please refer to its \sphinxhref{https://spark.apache.org/}{homepage}.


\section{Basic usage}
\label{\detokenize{usage/quickstart:basic-usage}}
Once you have chosen your preferred procedure, you only need to import the corresponding method
that you want to use as well as the types for your data, as you can see below:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.methods.DawidSkene}
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.types.MulticlassAnnotation}

\PYG{k}{val} \PYG{n}{exampleFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}examples/data/multi\PYGZhy{}ann.parquet\PYGZdq{}}

\PYG{k}{val} \PYG{n}{exampleData} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{exampleFile}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{MulticlassAnnotation}\PYG{o}{]}

\PYG{c+c1}{//Applying the learning algorithm}
\PYG{k}{val} \PYG{n}{mode} \PYG{k}{=} \PYG{n+nc}{DawidSkene}\PYG{o}{(}\PYG{n}{exampleData}\PYG{o}{)}

\PYG{c+c1}{//Get MulticlassLabel with the class predictions}
\PYG{k}{val} \PYG{n}{pred} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getMu}\PYG{o}{(}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{MulticlassLabel}\PYG{o}{]}

\PYG{c+c1}{//Annotator precision matrices}
\PYG{k}{val} \PYG{n}{annprec} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getAnnotatorPrecision}\PYG{o}{(}\PYG{o}{)}
\end{sphinxVerbatim}

Let’s go through each step of the code:
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
First we import the method, in this case \sphinxcode{\sphinxupquote{DawidSkene}} and the annotations type (\sphinxcode{\sphinxupquote{MulticlassAnnotation}}) that
we will need to load the data.

\item {} 
Then we load a data file (provided with the package) that contains annotations for different examples. We use
the method \sphinxcode{\sphinxupquote{as}} to convert the Spark DataFrame in a typed Spark Dataset (with type \sphinxcode{\sphinxupquote{MulticlassAnnotation}}).

\item {} 
To execute the model and obtain the result we use the model name directly.
This function returns a \sphinxcode{\sphinxupquote{DawidSkeneModel}}, that includes several methods to obtain results from the algorithm.

\item {} 
We use the  \sphinxcode{\sphinxupquote{getMu}} to obtain the ground truth estimations made by the model.

\item {} 
We use \sphinxcode{\sphinxupquote{getAnnotatorPrecision}} to obtain the annotator quality calculated by the model.

\end{enumerate}

You can consult the models implemented in this package in {\hyperref[\detokenize{package/methods:methods}]{\sphinxcrossref{\DUrole{std,std-ref}{Methods}}}}, where you will find a link to the
original article for the algorithm.


\chapter{Installation}
\label{\detokenize{usage/installation:installation}}\label{\detokenize{usage/installation:id1}}\label{\detokenize{usage/installation::doc}}
You can use our package in your own developments in three ways:
\begin{itemize}
\item {} 
Using the package directly using spark-packages

\item {} 
Adding it as a dependency to your project through Maven central.

\item {} 
Compiling the source code and using the \sphinxcode{\sphinxupquote{jar}} file.

\end{itemize}

Alternatively, if you just want to execute simple scala scripts locally,
you can use our docker image as explained in {\hyperref[\detokenize{usage/quickstart:quickstart}]{\sphinxcrossref{\DUrole{std,std-ref}{Quick Start}}}}


\section{Using Spark Packages}
\label{\detokenize{usage/installation:using-spark-packages}}
The easiest way of using the package is through \sphinxhref{https://spark-packages.org/}{Spark Packages}, as you only need to add the package in the command line when running your
application:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
spark\PYGZhy{}submit \PYGZhy{}\PYGZhy{}packages com.enriquegrodrigo:spark\PYGZhy{}crowd\PYGZus{}2.11:0.1.5 application.scala
\end{sphinxVerbatim}

You can also open a \sphinxtitleref{spark-shell} using:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
spark\PYGZhy{}shell \PYGZhy{}\PYGZhy{}packages com.enriquegrodrigo:spark\PYGZhy{}crowd\PYGZus{}2.11:0.1.5
\end{sphinxVerbatim}

Likewise, you can submit an application that uses \sphinxtitleref{spark-crowd} using:


\section{Adding it as a dependency}
\label{\detokenize{usage/installation:adding-it-as-a-dependency}}
In addition to Spark Packages, the package is also in Maven Central, so you can add the package as a dependency in your scala project.
For example, in \sphinxstyleemphasis{sbt} you can add the dependency as shown below:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{libraryDependencies} \PYG{o}{+=} \PYG{l+s}{\PYGZdq{}com.enriquegrodrigo\PYGZdq{}} \PYG{o}{\PYGZpc{}\PYGZpc{}} \PYG{l+s}{\PYGZdq{}spark\PYGZhy{}crowd\PYGZdq{}} \PYG{o}{\PYGZpc{}} \PYG{l+s}{\PYGZdq{}0.1.5\PYGZdq{}}
\end{sphinxVerbatim}

This will allow you to use the methods inside your Apache Spark projects.


\section{Compiling the source code}
\label{\detokenize{usage/installation:compiling-the-source-code}}
To build the package using \sphinxstyleemphasis{sbt} you can use the following command inside the spark-crowd folder:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
sbt package
\end{sphinxVerbatim}

It will generate a compiled \sphinxcode{\sphinxupquote{jar}} file that you can add to your project.


\chapter{Design and architechture}
\label{\detokenize{package/design:design-and-architechture}}\label{\detokenize{package/design::doc}}
The package design can be found in the figure below.

\noindent\sphinxincludegraphics{{package}.png}

Although, the library contains several folders, the only folders important for the users
are the \sphinxcode{\sphinxupquote{types}} folder, and the \sphinxcode{\sphinxupquote{methods}}. The other folders contain auxiliary
functions some of the methods. Concretely, in interesting to explore the data types, as
they are key to understanding how the package works, as well as the common interface of
the methods.


\section{Data types}
\label{\detokenize{package/design:data-types}}
We provide types for annotations datasets and ground truth datasets, as they usually follow
the same structure. These types are used in all the methods so you would need to convert
your annotations dataset the correct format accepted by the algorithm.

There are three types of annotations that we support for which we provide Scala case classes,
making it possible to detect errors at compile time when using the algorithms:
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{BinaryAnnotation}}: a Dataset of this type provides three columns, an example column, that
is the example for which the annotation is made, an annotator column, representing the
annotator that made the annotation and a value column, with the value of the annotation, that
can take value 0 or 1.

\item {} 
\sphinxcode{\sphinxupquote{MulticlassAnnotation}}: The difference form \sphinxcode{\sphinxupquote{BinaryAnnotation}} is that the value column can
take more than two values, in the range from 0 to the total number of values.

\item {} 
\sphinxcode{\sphinxupquote{RealAnnotation}}: In this case, the value column can take any numeric value.

\end{itemize}

You can convert an annotation dataframe with columns example, annotator and value to a
typed dataset easily with the following instruction:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{val} \PYG{n}{typedData} \PYG{k}{=} \PYG{n}{untypedData}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{RealAnnotation}\PYG{o}{]}
\end{sphinxVerbatim}

In the case of labels, we provide 5 types of labels, 2 of which are probabilistic. The three non probabilistic
types are:
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{BinaryLabel}}: represents a dataset of example, value pairs where value is a binary value (0 or 1).

\item {} 
\sphinxcode{\sphinxupquote{MulticlassLabel}}: where value can take more than two values.

\item {} 
\sphinxcode{\sphinxupquote{RealLabel}}: where value can take any numeric value.

\end{itemize}

The probabilistic types are used by some algorithms, to provide more information about the confidence of each
class value for an specific example.
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{BinarySoftLabel}}: represents a dataset with two columns: example, and probability (prob). For each example, the probability
of positive is given.

\item {} 
\sphinxcode{\sphinxupquote{MultiSoftLabel}}: representas a dataset with three columns: example, class and probability (prob). For each example, there will be
several entries depending on the number of classes of the problem, with the probability estimate.

\end{itemize}


\section{Methods}
\label{\detokenize{package/design:methods}}
All methods implemented are in the \sphinxcode{\sphinxupquote{methods}} package and are mostly independent of each other. There is only one exception to this, the
use of the MajorityVoting algorithms, as most of the algorithms used these methods in the initialization step. Apart from that, all logic
is implemented in their specific files.  This makes it easier to extend the package with new algorithms. Although independent, all algorithms have
a similar interface, which facilitates its use. To execute an algorithm, the user normally needs to use the \sphinxcode{\sphinxupquote{apply}} method of the model, as shown below

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\PYG{k}{val} \PYG{n}{model} \PYG{k}{=} \PYG{n+nc}{IBCC}\PYG{o}{(}\PYG{n}{annotations}\PYG{o}{)}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

After the model completes its execution, a model object is returned, which will have information about the ground truth estimations and
annotator’s quality and instance difficulties.

The only algorithm that do not follow this pattern is \sphinxcode{\sphinxupquote{MajorityVoting}}, which has methods for each of the class types and also to obtain
probabilistic labels. See the API Docs for details.


\chapter{Methods}
\label{\detokenize{package/methods:methods}}\label{\detokenize{package/methods:id1}}\label{\detokenize{package/methods::doc}}
You can find the methods implemented in this library below. All methods contain a link to its API where you
can find more information.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Methods implemented in spark-crowd}\label{\detokenize{package/methods:id8}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Binary
&\sphinxstyletheadfamily 
Multiclass
&\sphinxstyletheadfamily 
Real
&\sphinxstyletheadfamily 
Reference
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.MajorityVoting\$}{MajorityVoting}
&
\(\surd\)
&
\(\surd\)
&
\(\surd\)
&\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.DawidSkene\$}{DawidSkene}
&
\(\surd\)
&
\(\surd\)
&&
\sphinxhref{https://www.jstor.org/stable/2346806?seq=1\#page\_scan\_tab\_contents}{JRSS}
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.IBCC\$}{IBCC}
&
\(\surd\)
&
\(\surd\)
&&
\sphinxhref{http://proceedings.mlr.press/v22/kim12.html}{AISTATS}
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.GLAD\$}{GLAD}
&
\(\surd\)
&&&
\sphinxhref{https://papers.nips.cc/paper/3644-whose-vote-should-count-more-optimal-integration-of-labels-from-labelers-of-unknown-expertise}{NIPS}
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.CGLAD\$}{CGLAD}
&
\(\surd\)
&&&
\sphinxhref{https://aida.ii.uam.es/ideal2018/\#!/main}{IDEAL}
\\
\hline
Raykar
&
\(\surd\)
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.RaykarBinary\$}{RaykarBinary}
&
\(\surd\)
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.RaykarBinary\$}{RaykarMulti}
&
\(\surd\)
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.RaykarBinary\$}{RaykarCont}
&
\sphinxhref{http://jmlr.csail.mit.edu/papers/v11/raykar10a.html}{JMLR}
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.CATD\$}{CATD}
&&&
\(\surd\)
&
\sphinxhref{http://www.vldb.org/pvldb/vol8/p425-li.pdf}{VLDB}
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.PM\$}{PM}
&&&
\(\surd\)
&
\sphinxhref{https://dl.acm.org/citation.cfm?id=2588555.2610509}{SIGMOD}
\\
\hline
\sphinxhref{https://enriquegrodrigo.github.io/spark-crowd/\#com.enriquegrodrigo.spark.crowd.methods.PMTI\$}{PMTI}
&&&
\(\surd\)
&
\sphinxhref{http://www.vldb.org/pvldb/vol10/p541-zheng.pdf}{VLDB2}
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Below, we provide a short summary of each method. However, to understand the method completely we suggest the
user to study the reference.


\section{MajorityVoting}
\label{\detokenize{package/methods:id2}}
With this, we refer to the mean for continuous target variables and the most frequent class for the discrete
case. Expressing this methods in terms of annotator accuracy, these methods suppose that all annotators have
the same experience. Therefore, their contributions are weighted equally. Apart from the classical mean and
most frequent class, we also provide methods that return the proportion of each class value for each example.
See the API Docs for more information on these methods.


\section{DawidSkene}
\label{\detokenize{package/methods:id3}}
This method estimates the accuracy of the annotators from the annotations themselves. For this, it uses the EM
algorithm, starting from the most frequent class and improving the estimations through several iterations. The
algorithm returns both the estimation of the ground truth and the accuracy of these annotators (a confusion
matrix for each). This algorithm is a good alternative when looking for a simple way of aggregating annotations
without the assumption that all annotators are equally accurate.


\section{IBCC}
\label{\detokenize{package/methods:id4}}
This method is similar to the previous one but uses probabilistic estimations for the classes. For each example,
the model returns probabilities for each class, so they can be useful in problems where a probability is needed.
Both in our tests and in the test \sphinxhref{https://zhydhkcws.github.io/crowd\_truth\_inference/index.html}{here}, so it is
a good compromise between the complexity of the model and its performance.


\section{GLAD}
\label{\detokenize{package/methods:id5}}
This method estimates both the accuracy of the annotators (one parameter per annotator) and the difficulty
of each example (a parameter for each instance), through EM algorithm and gradient descent. This complexity
comes at a cost of a slower algorithm in general, but it is one of the only two algorithms implemented capable of estimating these two parameters.


\section{CGLAD}
\label{\detokenize{package/methods:id6}}
This method is an enhancement over the original GLAD algorithm to tackle bigger datasets more easily, using
clustering techniques over the examples to recude the number of parameters to be estimated, following a similar
learning process to GLAD algorithm.


\section{Raykar’s algorithms}
\label{\detokenize{package/methods:raykar-s-algorithms}}
We implement the three methods proposed in the paper Learning from crowds (referenced in the table) for learning
from crowdsourced data when features are available. These methods use an annotations matrix, as the previous ones,
but also a feature matrix, with the features for each instance. Then, the algorithms infer together a logistic
model, for the discrete case, or a regression model, for the continuous case, the ground truth from the data,
and the quality of the annotators, with are returned from the methods in our package.


\section{CATD}
\label{\detokenize{package/methods:id7}}
This method estimates both the quality of the annotators (as a weight in the aggregation) and the ground truth
for continuous target variables. It only uses the annotations for the aggregation, learning from them which
annotators should be more trusted, assigning more weight to them, for the aggregation. In the package, only
the continuous version is implemented as other algorithms seem to work better in the discrete cases (see \sphinxhref{https://zhydhkcws.github.io/crowd\_truth\_inference/index.html}{this paper} for more information)


\section{PM and PMTI}
\label{\detokenize{package/methods:pm-and-pmti}}
Another method for continuous target variables. We implement two versions, one following the formulas appearing
in the original paper and the modification implemented in  \sphinxhref{https://zhydhkcws.github.io/crowd\_truth\_inference/index.html}{this package}. This modification seems to obtain better results in our experimentation (you can check it in \DUrole{xref,std,std-ref}{comparison}.


\chapter{Examples}
\label{\detokenize{usage/examples:examples}}\label{\detokenize{usage/examples::doc}}
In this page we provide examples for several of the algorithms in the library.
You can find the data used for the examples in the Github repository.


\section{MajorityVoting}
\label{\detokenize{usage/examples:majorityvoting}}
Let’s start with the simpler algorithm to illustrate how to use the library, MajorityVoting:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.methods.MajorityVoting}
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.types.BinaryAnnotation}

\PYG{k}{val} \PYG{n}{exampleFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}data/binary\PYGZhy{}ann.parquet\PYGZdq{}}

\PYG{k}{val} \PYG{n}{exampleDataBinary} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{exampleFile}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{BinaryAnnotation}\PYG{o}{]}

\PYG{k}{val} \PYG{n}{muBinary} \PYG{k}{=} \PYG{n+nc}{MajorityVoting}\PYG{o}{.}\PYG{n}{transformBinary}\PYG{o}{(}\PYG{n}{exampleDataBinary}\PYG{o}{)}

\PYG{n}{muBinary}\PYG{o}{.}\PYG{n}{show}\PYG{o}{(}\PYG{o}{)}
\end{sphinxVerbatim}

This will show a result similar to this one:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+}
\PYG{o}{\textbar{}}\PYG{n}{example}\PYG{o}{\textbar{}}\PYG{n}{value}\PYG{o}{\textbar{}}
\PYG{o}{+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+}
\PYG{o}{\textbar{}}     \PYG{l+m+mi}{26}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{0}\PYG{o}{\textbar{}}
\PYG{o}{\textbar{}}     \PYG{l+m+mi}{29}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{1}\PYG{o}{\textbar{}}
\PYG{o}{\textbar{}}    \PYG{l+m+mi}{474}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{0}\PYG{o}{\textbar{}}
\PYG{o}{\textbar{}}    \PYG{l+m+mi}{964}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{1}\PYG{o}{\textbar{}}
\PYG{o}{\textbar{}}     \PYG{l+m+mi}{65}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{0}\PYG{o}{\textbar{}}
\PYG{o}{\textbar{}}    \PYG{l+m+mi}{191}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{0}\PYG{o}{\textbar{}}
\PYG{o}{\textbar{}}    \PYG{l+m+mi}{418}\PYG{o}{\textbar{}}    \PYG{l+m+mi}{1}\PYG{o}{\textbar{}}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

MajorityVoting algorithms suppose that all annotators are equally accurate, so they choose the
most frequent annotation as the ground truth label. Therefore, they only return the ground
truth for the problem.

The data file in this example follow the format from the \sphinxcode{\sphinxupquote{BinaryAnnotation}} type:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{example}\PYG{o}{,} \PYG{n}{annotator}\PYG{o}{,} \PYG{n}{value}
      \PYG{l+m+mi}{0}\PYG{o}{,}         \PYG{l+m+mi}{0}\PYG{o}{,}     \PYG{l+m+mi}{1}
      \PYG{l+m+mi}{0}\PYG{o}{,}         \PYG{l+m+mi}{1}\PYG{o}{,}     \PYG{l+m+mi}{0}
      \PYG{l+m+mi}{0}\PYG{o}{,}         \PYG{l+m+mi}{2}\PYG{o}{,}     \PYG{l+m+mi}{1}
      \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

In this example, we use a \sphinxcode{\sphinxupquote{.parquet}} data file, which is usually a good option in terms of
efficiency. However, we do not limit the types of files you can use, as long as they can be
converted to typed datasets of \sphinxcode{\sphinxupquote{BinaryAnnotation}}, \sphinxcode{\sphinxupquote{MulticlassAnnotation}} or \sphinxcode{\sphinxupquote{RealAnnotation}}.
However, algorithms will suppose that there are no missing examples or annotators.

Concretely, MajorityVoting object can make predictions both for discrete classes (\sphinxcode{\sphinxupquote{BinaryAnnotation}} and
\sphinxcode{\sphinxupquote{MulticlassAnnotation}}) and continuous-valued target variables. (\sphinxcode{\sphinxupquote{RealAnnotation}}). You can find
information about these methods in the \sphinxhref{\_static/api/index.html}{API Docs}.


\section{DawidSkene}
\label{\detokenize{usage/examples:dawidskene}}
This algorithm is one of the most recommended for its ease of use as well as for it capabilities. It does not
have a great number of free parameters and obtains good results usually.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.methods.DawidSkene}
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.types.MulticlassAnnotation}

\PYG{k}{val} \PYG{n}{exampleFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}examples/data/multi\PYGZhy{}ann.parquet\PYGZdq{}}

\PYG{k}{val} \PYG{n}{exampleData} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{exampleFile}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{MulticlassAnnotation}\PYG{o}{]}

\PYG{k}{val} \PYG{n}{mode} \PYG{k}{=} \PYG{n+nc}{DawidSkene}\PYG{o}{(}\PYG{n}{exampleData}\PYG{o}{,} \PYG{n}{eMIters}\PYG{k}{=}\PYG{l+m+mi}{10}\PYG{o}{,} \PYG{n}{emThreshold}\PYG{k}{=}\PYG{l+m+mf}{0.001}\PYG{o}{)}

\PYG{k}{val} \PYG{n}{pred} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getMu}\PYG{o}{(}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{MulticlassLabel}\PYG{o}{]}

\PYG{k}{val} \PYG{n}{annprec} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getAnnotatorPrecision}\PYG{o}{(}\PYG{o}{)}
\end{sphinxVerbatim}

In our implementation, we use 2 parameters for controlling the algorithm execution, the maximum number
of EM iterations and the threshold for the likelihood change. The execution will stop if the it has reach
the established iterations or if the change in likelihood is less than the threshold. You do not need to
provided these parameters, as they have default values.

One executed, the model will provide us with an estimation of the ground truth, taking into account the
annotations and the quality of each annotator. We can access this information as shown on the example.
Concretely, the provided annotator precision is a three dimensional array with, first dimension representing
the annotator and the second and third, the confusion matrix for the annotator.


\section{GLAD}
\label{\detokenize{usage/examples:glad}}
The GLAD algorithm is interesting as it provides both annotator accuracies and example difficulties obtained
solely from the annotations. Here is an example of how to use it:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.methods.Glad}
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.types.BinaryAnnotation}

\PYG{k}{val} \PYG{n}{annFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}data/binary\PYGZhy{}ann.parquet\PYGZdq{}}

\PYG{k}{val} \PYG{n}{annData} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{annFile}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{BinaryAnnotation}\PYG{o}{]}

\PYG{k}{val} \PYG{n}{mode} \PYG{k}{=} \PYG{n+nc}{Glad}\PYG{o}{(}\PYG{n}{annData}\PYG{o}{,}
                  \PYG{n}{eMIters}\PYG{k}{=}\PYG{l+m+mi}{5}\PYG{o}{,} \PYG{c+c1}{//Maximum number of iterations of EM algorithm}
                  \PYG{n}{eMThreshold}\PYG{k}{=}\PYG{l+m+mf}{0.1}\PYG{o}{,} \PYG{c+c1}{//Threshold for likelihood changes}
                  \PYG{n}{gradIters}\PYG{k}{=}\PYG{l+m+mi}{30}\PYG{o}{,} \PYG{c+c1}{//Gradient descent max number of iterations}
                  \PYG{n}{gradTreshold}\PYG{k}{=}\PYG{l+m+mf}{0.5}\PYG{o}{,} \PYG{c+c1}{//Gradient descent threshold}
                  \PYG{n}{gradLearningRate}\PYG{k}{=}\PYG{l+m+mf}{0.01}\PYG{o}{,} \PYG{c+c1}{//Gradient descent learning rate}
                  \PYG{n}{alphaPrior}\PYG{k}{=}\PYG{l+m+mi}{1}\PYG{o}{,} \PYG{c+c1}{//Alpha first value (GLAD specific)}
                  \PYG{n}{betaPrior}\PYG{k}{=}\PYG{l+m+mi}{1}\PYG{o}{)} \PYG{c+c1}{//Beta first value (GLAD specific)}

\PYG{k}{val} \PYG{n}{pred} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getMu}\PYG{o}{(}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{BinarySoftLabel}\PYG{o}{]}

\PYG{k}{val} \PYG{n}{annprec} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getAnnotatorPrecision}\PYG{o}{(}\PYG{o}{)}

\PYG{k}{val} \PYG{n}{annprec} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getInstanceDifficulty}\PYG{o}{(}\PYG{o}{)}
\end{sphinxVerbatim}

This model as it is implemented in the library is only compatible with binary class problems. It has a
higher number of free parameters in comparison with the previous algorithm, but we provided default
values for all of them for convenience. The meaning of each of these parameters is commented in the
example above, as it is on the documentation. The annotator precision is given in a vector, with an
entry for each annotator. The difficulty is given in the form of a DataFrame, returning
a difficulty value for each example. For more information about this you can consult the documentation
and/or the paper.


\section{RaykarBinary, RaykarMulti and RaykarCont}
\label{\detokenize{usage/examples:raykarbinary-raykarmulti-and-raykarcont}}
We implement the three variants of this algorithm, for discrete and continuous target variables.
These algorithms have in common that they are able to use features to estimate the ground truth
and even learn a linear model. The model also is able to use prior information about annotators,
which can be useful to add more confidence to certain annotators. In the next example we show
how to use this model adding a prior that indicates that we trust a lot in the first annotator
and that we now that the second annotator is not reliable.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.methods.RaykarBinary}
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.types.BinaryAnnotation}

\PYG{k}{val} \PYG{n}{exampleFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}data/binary\PYGZhy{}data.parquet\PYGZdq{}}
\PYG{k}{val} \PYG{n}{annFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}data/binary\PYGZhy{}ann.parquet\PYGZdq{}}

\PYG{k}{val} \PYG{n}{exampleData} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{exampleFile}\PYG{o}{)}
\PYG{k}{val} \PYG{n}{annData} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{annFile}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{BinaryAnnotation}\PYG{o}{]}

\PYG{c+c1}{//Preparing priors}
\PYG{k}{val} \PYG{n}{nAnn} \PYG{k}{=} \PYG{n}{annData}\PYG{o}{.}\PYG{n}{map}\PYG{o}{(}\PYG{k}{\PYGZus{}}\PYG{o}{.}\PYG{n}{annotator}\PYG{o}{)}\PYG{o}{.}\PYG{n}{distinct}\PYG{o}{.}\PYG{n}{count}\PYG{o}{(}\PYG{o}{)}\PYG{o}{.}\PYG{n}{toInt}

\PYG{k}{val} \PYG{n}{a} \PYG{k}{=} \PYG{n+nc}{Array}\PYG{o}{.}\PYG{n}{fill}\PYG{o}{[}\PYG{k+kt}{Double}\PYG{o}{]}\PYG{o}{(}\PYG{n}{nAnn}\PYG{o}{,}\PYG{l+m+mi}{2}\PYG{o}{)}\PYG{o}{(}\PYG{l+m+mf}{2.0}\PYG{o}{)} \PYG{c+c1}{//Uniform prior}
\PYG{k}{val} \PYG{n}{b} \PYG{k}{=} \PYG{n+nc}{Array}\PYG{o}{.}\PYG{n}{fill}\PYG{o}{[}\PYG{k+kt}{Double}\PYG{o}{]}\PYG{o}{(}\PYG{n}{nAnn}\PYG{o}{,}\PYG{l+m+mi}{2}\PYG{o}{)}\PYG{o}{(}\PYG{l+m+mf}{2.0}\PYG{o}{)} \PYG{c+c1}{//Uniform prior}

\PYG{c+c1}{//Give first annotator more confidence}
\PYG{n}{a}\PYG{o}{(}\PYG{l+m+mi}{0}\PYG{o}{)}\PYG{o}{(}\PYG{l+m+mi}{0}\PYG{o}{)} \PYG{o}{+=} \PYG{l+m+mi}{1000}
\PYG{n}{b}\PYG{o}{(}\PYG{l+m+mi}{0}\PYG{o}{)}\PYG{o}{(}\PYG{l+m+mi}{0}\PYG{o}{)} \PYG{o}{+=} \PYG{l+m+mi}{1000}

\PYG{c+c1}{//Give second annotator less confidence}
\PYG{c+c1}{//Annotator 1}
\PYG{n}{a}\PYG{o}{(}\PYG{l+m+mi}{1}\PYG{o}{)}\PYG{o}{(}\PYG{l+m+mi}{1}\PYG{o}{)} \PYG{o}{+=} \PYG{l+m+mi}{1000}
\PYG{n}{b}\PYG{o}{(}\PYG{l+m+mi}{1}\PYG{o}{)}\PYG{o}{(}\PYG{l+m+mi}{1}\PYG{o}{)} \PYG{o}{+=} \PYG{l+m+mi}{1000}


\PYG{c+c1}{//Applying the learning algorithm}
\PYG{k}{val} \PYG{n}{mode} \PYG{k}{=} \PYG{n+nc}{RaykarBinary}\PYG{o}{(}\PYG{n}{exampleData}\PYG{o}{,} \PYG{n}{annData}\PYG{o}{,}
                          \PYG{n}{eMIters}\PYG{k}{=}\PYG{l+m+mi}{5}\PYG{o}{,}
                          \PYG{n}{eMThreshold}\PYG{k}{=}\PYG{l+m+mf}{0.001}\PYG{o}{,}
                          \PYG{n}{gradIters}\PYG{k}{=}\PYG{l+m+mi}{100}\PYG{o}{,}
                          \PYG{n}{gradThreshold}\PYG{k}{=}\PYG{l+m+mf}{0.1}\PYG{o}{,}
                          \PYG{n}{gradLearning}\PYG{k}{=}\PYG{l+m+mf}{0.1}
                          \PYG{n}{a\PYGZus{}prior}\PYG{k}{=}\PYG{n+nc}{Some}\PYG{o}{(}\PYG{n}{a}\PYG{o}{)}\PYG{o}{,} \PYG{n}{b\PYGZus{}prior}\PYG{k}{=}\PYG{n+nc}{Some}\PYG{o}{(}\PYG{n}{b}\PYG{o}{)}\PYG{o}{)}

\PYG{c+c1}{//Get MulticlassLabel with the class predictions}
\PYG{k}{val} \PYG{n}{pred} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getMu}\PYG{o}{(}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{BinarySoftLabel}\PYG{o}{]}

\PYG{c+c1}{//Annotator precision matrices}
\PYG{k}{val} \PYG{n}{annprec} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{getAnnotatorPrecision}\PYG{o}{(}\PYG{o}{)}
\end{sphinxVerbatim}

Apart form the features matrix and the priors, the meaning of the parameters is the same as in the previous examples.
The priors are matrices of A by 2. In each row we have the hyperparameters of a Beta distribution for each annotator.
The \sphinxcode{\sphinxupquote{a\_prior}} gives prior information about the ability of annotators to classify correctly a positive example. The
\sphinxcode{\sphinxupquote{b\_prior}} does the same thing but for the negative examples. More information about this method as well as the methods
for discrete and continuous target variables can be found in the API docs.


\section{CATD}
\label{\detokenize{usage/examples:catd}}
This method allows to estimate continuous-value target variables from annotations.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.methods.CATD}
\PYG{k}{import} \PYG{n+nn}{com.enriquegrodrigo.spark.crowd.types.RealAnnotation}

\PYG{n}{sc}\PYG{o}{.}\PYG{n}{setCheckpointDir}\PYG{o}{(}\PYG{l+s}{\PYGZdq{}checkpoint\PYGZdq{}}\PYG{o}{)}

\PYG{k}{val} \PYG{n}{annFile} \PYG{k}{=} \PYG{l+s}{\PYGZdq{}examples/data/cont\PYGZhy{}ann.parquet\PYGZdq{}}

\PYG{k}{val} \PYG{n}{annData} \PYG{k}{=} \PYG{n}{spark}\PYG{o}{.}\PYG{n}{read}\PYG{o}{.}\PYG{n}{parquet}\PYG{o}{(}\PYG{n}{annFile}\PYG{o}{)}\PYG{o}{.}\PYG{n}{as}\PYG{o}{[}\PYG{k+kt}{RealAnnotation}\PYG{o}{]}

\PYG{c+c1}{//Applying the learning algorithm}
\PYG{k}{val} \PYG{n}{mode} \PYG{k}{=} \PYG{n+nc}{CATD}\PYG{o}{(}\PYG{n}{annData}\PYG{o}{,} \PYG{n}{iterations}\PYG{k}{=}\PYG{l+m+mi}{5}\PYG{o}{,}
                          \PYG{n}{threshold}\PYG{k}{=}\PYG{l+m+mf}{0.1}\PYG{o}{,}
                          \PYG{n}{alpha}\PYG{k}{=}\PYG{l+m+mf}{0.05}\PYG{o}{)}

\PYG{c+c1}{//Get MulticlassLabel with the class predictions}
\PYG{k}{val} \PYG{n}{pred} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{mu}

\PYG{c+c1}{//Annotator precision matrices}
\PYG{k}{val} \PYG{n}{annprec} \PYG{k}{=} \PYG{n}{mode}\PYG{o}{.}\PYG{n}{weights}
\end{sphinxVerbatim}

It returns a model from which you can get the ground truth estimation and
also the annotator weight used (more weight would signify a better annotator).
The algorithm uses parameters such as \sphinxcode{\sphinxupquote{iterations}} and \sphinxcode{\sphinxupquote{threshold}} for
controlling the execution, and also \sphinxcode{\sphinxupquote{alpha}}, which is a parameter of the model
(check the API docs for more information).


\chapter{Comparison with other packages}
\label{\detokenize{package/other:comparison-with-other-packages}}\label{\detokenize{package/other:comparison}}\label{\detokenize{package/other::doc}}
There exists other packages implementing similar methods in other languages, but with
different goals in mind. To our knowledge, there are 2 software packages with the goal
of learning from crowdsourced data:
\begin{itemize}
\item {} 
\sphinxhref{http://ceka.sourceforge.net/}{Ceka}: it is a Java software package based on WEKA, with
a great number of methods that can be used to learn from crowdsource data.

\item {} 
\sphinxhref{https://zhydhkcws.github.io/crowd\_truth\_inference/index.html/}{Truth inference in Crowdsourcing} makes available a collection
of methods in Python to learn from crowdsourced data.

\end{itemize}

Both are useful packages when dealing with crowdsourced data, with a focus on research. \sphinxtitleref{spark-crowd} is different, in the sense that
not only is useful in research, but in production as well, providing tests for all of its methods with a high test coverage. Moreover,
methods have been implemented with a focus on scalability, so it is useful in a wide variety of situations. We provide a
comparison of the methods over a set of datasets next, taking into account both quality of the models and execution time.


\section{Data}
\label{\detokenize{package/other:data}}
For this performance test we use simulated datasets of increasing size:
\begin{itemize}
\item {} 
\sphinxstylestrong{binary1-4}: simulated binary class datasets with 10K, 100K, 1M and 10M instances respectively. Each of them
has 10 simulated annotations per instance, and the ground truth for each example is known (but not used in the
learning process). The accuracy shown in the tables is obtained over this known ground truth.

\item {} 
\sphinxstylestrong{cont1-4}: simulated continuous target variable datasets, with 10k, 100k, 1M and 10M instances respectively. Each of them
has 10 simulated annotations per instance, and the ground truth for each example is known (but not used in the
learning process). The Mean Absolute Error is obtained over this known ground truth.

\item {} 
\sphinxstylestrong{crowdscale}. A real multiclass dataset from the \sphinxstyleemphasis{Crowdsourcing at Scale} challenge. The data is comprised of 98979 instances,
evaluated by, at least, 5 annotators, for a total of 569375 answers. We only have ground truth for the 0.3\% of the data,
which is used for evaluation.

\end{itemize}

All datasets are available through this \sphinxhref{https://www.dropbox.com/sh/odmhdf83latvezu/AAB6om3Oy7-waf-msIvk9yX6a?dl=0}{link}


\section{CEKA}
\label{\detokenize{package/other:id1}}
To compare our methods with Ceka, we used two of the main methods implemented in both packages, MajorityVoting and DawidSkene. Ceka and
spark-crowd also implement GLAD and Raykar’s algorithms. However, in Ceka, these algorithms are implemented using wrappers to other libraries.
The library for the GLAD algorithm is not available on our platform, as it is given as an EXE Windows file, and the wrapper for Raykar’s algorithms
does not admit any configuration parameters.

We provide the results of the execution of these methods in terms of accuracy (Acc) and time (in seconds). For our package, we also include
the execution time for a cluster (tc) with 3 executor nodes of 10 cores and 30Gb of memory each.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Comparison with Ceka}\label{\detokenize{package/other:id3}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily MajorityVoting
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily DawidSkene
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Ceka
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Ceka
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
binary1
&
0.931
&
21
&
0.931
&
11
&
7
&
0.994
&
57
&
0.994
&
31
&
32
\\
\hline
binary2
&
0.936
&
15983
&
0.936
&
11
&
7
&
0.994
&
49259
&
0.994
&
60
&
51
\\
\hline
binary3
&
X
&
X
&
0.936
&
21
&
8
&
X
&
X
&
0.994
&
111
&
69
\\
\hline
binary4
&
X
&
X
&
0.936
&
84
&
42
&
X
&
X
&
0.994
&&\\
\hline
crowdscale
&
0.88
&
10458
&
0.9
&
13
&
7
&
0.89
&
30999
&
0.9033
&
447
&
86
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Regarding accuracy, both packages achieve comparable results. However, regarding execution time, spark-crowd obtains
significantly better results among all datasets especially on the bigger datasets, where it can solve problems that
Ceka is not able to. You can see the speedup results in the table below.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Speedup in comparison to Ceka}\label{\detokenize{package/other:id4}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{4}%
\begin{varwidth}[t]{\sphinxcolwidth{4}{5}}
\sphinxstyletheadfamily MajorityVoting  \textbar{}      DawidSkene
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
binary1
&
1.86
&
2.93
&
1.84
&
1.78
\\
\hline
binary2
&
1453
&
2283
&
272
&
1146
\\
\hline
crowdscale
&
804
&
1494
&
69
&
360
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

We can see that spark-crowd obtains a high speedup in bigger datasets and performs
slightly better in the smaller ones.


\section{Truth inference in crowdsourcing}
\label{\detokenize{package/other:id2}}
Now we compare spark-crowd with the methods available in this paper. Although the methods
can certainly be used for to compare and try the algorithms, the integration of these
methods into a large ecosystem will be very difficult, as the authors do not provide
a software package structure. However, as it is an available package with a great number
of methods, a comparison with them is needed. We will use the same datasets
as the ones used in the previous comparison. In this case, we can compare a higher
number of models, as most of the methods are written in python. However, we were only able
to execute the methods over datasets with binary or continuous target variables. As far as we
know, the use of multiclass target variables seems to not be possible. Moreover, the use of
feature sets is also restricted, although algorithms that should be capable of dealing with
this kind of data are implemented, as is the case with the Raykar’s methods.

First, we compare the algorithms capable of learning from binary classes without feature sets.
Inside this category, we will compare MajorityVoting, DawidSkene, GLAD and IBCC. For each dataset, we show
results in terms of Accuracy (Acc) and time (in seconds). The table below shows the results for
MajorityVoting and DawidSkene. Both packages obtain the same results in terms of
accuracy. For the smaller datasets, the overhead imposed by parallelism makes Truth-inf a better choice,
at least in terms of execution time. However, as the datasets increase, and specially, in the last two
cases, the speedup obtained by our algorithm is notable. In the case of DawidSkene, the Truth-inf
package is not able to complete the execution because of memory constraints in the largest dataset.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Comparative with Truth inference in Crowdsourcing package}\label{\detokenize{package/other:id5}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily MajorityVoting
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily DawidSkene
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
binary1
&
0.931
&
1
&
0.931
&
11
&
7
&
0.994
&
12
&
0.994
&
31
&
32
\\
\hline
binary2
&
0.936
&
8
&
0.936
&
11
&
7
&
0.994
&
161
&
0.994
&
60
&
51
\\
\hline
binary3
&
0.936
&
112
&
0.936
&
21
&
8
&
0.994
&
1705
&
0.994
&
111
&
69
\\
\hline
binary4
&
0.936
&
2908
&
0.936
&
13
&
7
&
M
&
M
&
0.994
&
703
&
426
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Next we show the results for GLAD and IBCC. As the user can see, both packages obtain similar results
in terms of accuracy. Regarding execution time, both packages obtain comparable results
in the two smaller datasets (with a slight speedup in the \sphinxcode{\sphinxupquote{binary2}}) for the GLAD algorithm. However,
Truth-inf is not able to complete the execution for the two largest datasets.
In the case of IBCC, the speedup starts to be noticeable from the second dataset. Again, Truth-inf was
unable to complete the execution in a reasonable ammount of time for the last dataset.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Comparative with Truth inference in Crowdsourcing package (2)}\label{\detokenize{package/other:id6}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily GLAD
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily IBCC
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
binary1
&
0.994
&
1185
&
0.994
&
1568
&
1547
&
0.994
&
22
&
0.994
&
74
&
67
\\
\hline
binary2
&
0.994
&
4168
&
0.994
&
2959
&
2051
&
0.994
&
372
&
0.994
&
97
&
76
\\
\hline
binary3
&
X
&
X
&
0.491
&
600
&
226
&
0.994
&
25764
&
0.994
&
203
&
129
\\
\hline
binary4
&
X
&
X
&
0.974
&
2407
&
1158
&
X
&
X
&
X
&
1529
&
823
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

A thing to notice regarding the last execution of this algorithm is that at large scale, the performance of the
algorithm seems to degrade. This may be due to the ammount of parameters the algorithm needs to estimate (for the
difficulty, one for every example). A way to improve the estimation goes through decrease the learning rate, which
makes the algorithm slower, as it needs a lot more iterations to obtain a good solution. This makes the algorithm
unsuitable for several big data contexts.  To tackle this kind of problems, we developed and enhancement, CGlad, recently
published and which is included in the package (See the last section of this page for results of other
methods in the package, as well as this enhancement)

Next we analize methods that are able to learn from continuous target variables: MajorityVoting (mean), CATD and PM (with mean initialization). We show the results in terms of MAE (Mean absolute error) and time (in seconds). The
results for MajorityVoting and CATD can be found below in the table below.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Comparative with Truth inference in Crowdsourcing package on continuous target variables}\label{\detokenize{package/other:id7}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily MajorityVoting (mean)
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily CATD
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
cont1
&
1.234
&
1
&
1.234
&
6
&
8
&
0.324
&
207
&
0.324
&
25
&
28
\\
\hline
cont2
&
1.231
&
8
&
1.231
&
7
&
9
&
0.321
&
10429
&
0.321
&
26
&
24
\\
\hline
cont3
&
1.231
&
74
&
1.231
&
12
&
13
&
X
&
X
&
0.322
&
42
&
38
\\
\hline
cont4
&
1.231
&
581
&
1.231
&
56
&
23
&
X
&
X
&
0.322
&
247
&
176
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

As you can see in the table, both packages obtain similar results regarding MAE. Regarding performance,
MajorityVoting is quite performant in the Truth-inf package, specially in the smaller dataset. For smaller datasets,
the increase overhead impose by parallelism makes the execution time of our package a little worse in comparison.
However, as the dataset increase in size, the speedup obtained by our package is notable, even in this algorithm,
which is less complex computationally. Regarding CATD, Truth-inf seems not to be able to solve the 2 bigger problems
in a reasonable time, however, they can be solved by our package in a small ammount of time. Even for the smaller
datasets, our package obtains a high speedup in comparison to Truth-inf.

In the table below you can find the results for PM and PMTI algorithms.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Comparative with Truth inference in Crowdsourcing package on continuous target variables (2)}\label{\detokenize{package/other:id8}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily PM
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{5}%
\begin{varwidth}[t]{\sphinxcolwidth{5}{11}}
\sphinxstyletheadfamily PMTI
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{11}}
\sphinxstyletheadfamily Truth-inf
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{11}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
cont1
&
0.495
&
77
&
0.495
&
57
&
51
&
0.388
&
139
&
0.388
&
68
&
61
\\
\hline
cont2
&
0.493
&
8079
&
0.495
&
76
&
57
&
0.386
&
14167
&
0.386
&
74
&
58
\\
\hline
cont3
&
X
&
X
&
0.494
&
130
&
97
&
X
&
X
&
0.387
&
143
&
98
\\
\hline
cont4
&
X
&
X
&
0.494
&
769
&
421
&
X
&
X
&
0.387
&
996
&
475
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Although similar, the modification implemented in Truth-inf from the original algorithm seems to be more
accurate. The code for the original version was also available, although it was commented in the source code.
Even in the smaller dataset, our package obtains a slight speedup. However as the datasets increase in size,
our package is able to obtain a much higher speedup. As was the case with CATD, it was impossible for us to
solve them in a reasonable ammount of time with Truth-inf.


\section{Other methods}
\label{\detokenize{package/other:other-methods}}
Experimentation will not be complete without looking at the other methods implemented by our package that
are not directly implemented by the packages above. These methods are the full implementation of the Raykar’s
algorithms (taking into account the features of the instances) and the enhancement over the GLAD algorithm. As a
note, Truth-inf implements a version of Raykar’s algorithms that do not use the features of the instances. First,
we show the results obtained by the Raykar’s methods for discrete target variables.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Other methods implemented in spark-crowd. Raykar’s methods for discrete target variables.}\label{\detokenize{package/other:id9}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{7}}
\sphinxstyletheadfamily RaykarBinary
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{7}}
\sphinxstyletheadfamily RaykarMulti
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{7}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{7}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
binary1
&
0.994
&
65
&
63
&
0.994
&
167
&
147
\\
\hline
binary2
&
0.994
&
92
&
74
&
0.994
&
241
&
176
\\
\hline
binary3
&
0.994
&
181
&
190
&
0.994
&
532
&
339
\\
\hline
binary4
&
0.994
&
1149
&
560
&
0.994
&
4860
&
1196
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Next we show the Raykar method for tackling continous target variables.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Other methods implemented in spark-crowd. Raykar method for continuous target variables.}\label{\detokenize{package/other:id10}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{4}%
\begin{varwidth}[t]{\sphinxcolwidth{4}{5}}
\sphinxstyletheadfamily RaykarCont
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{4}%
\begin{varwidth}[t]{\sphinxcolwidth{4}{5}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{5}}
\sphinxstyletheadfamily t1
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&\sphinxstyletheadfamily 
tc
\\
\hline
cont1
&
0.994
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{5}}
31
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&
32
\\
\hline
cont2
&
0.994
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{5}}
60
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&
51
\\
\hline
cont3
&
0.994
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{5}}
111
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&
69
\\
\hline
cont4
&
0.994
&\sphinxstartmulticolumn{2}%
\begin{varwidth}[t]{\sphinxcolwidth{2}{5}}
703
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
&
426
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Lastly, we show the results for the CGlad algorithm. As you can see, it obtains similar results to the GLAD algorithm
but it performs better in the larger cases.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxcaption{Other methods implemented in spark-crowd. CGlad, an enhancement over Glad algorithm.}\label{\detokenize{package/other:id11}}
\sphinxaftercaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily &\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{4}}
\sphinxstyletheadfamily CGlad
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily &\sphinxstartmulticolumn{3}%
\begin{varwidth}[t]{\sphinxcolwidth{3}{4}}
\sphinxstyletheadfamily spark-crowd
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
\sphinxstopmulticolumn
\\
\hline\sphinxstyletheadfamily 
Method
&\sphinxstyletheadfamily 
Acc
&\sphinxstyletheadfamily 
t1
&\sphinxstyletheadfamily 
tc
\\
\hline
binary1
&
0.994
&
128
&
128
\\
\hline
binary2
&
0.995
&
233
&
185
\\
\hline
binary3
&
0.995
&
1429
&
607
\\
\hline
binary4
&
0.995
&
17337
&
6190
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\chapter{Contributors}
\label{\detokenize{package/contributors:contributors}}\label{\detokenize{package/contributors::doc}}
We are open to contributions in the form of bugs reports, enhancements or even new algorithms.


\section{Bug reports}
\label{\detokenize{package/contributors:bug-reports}}
Bugs are tracked using Github issues. When creating a bug report, try to provide as much information as possible to help maintainers
reproduce the problem
\begin{itemize}
\item {} 
Use a clear and descriptive title for the issue to identify the problem.

\item {} 
Describe the exact steps which reproduce the problem in as many details as possible. For example, start by explaining how you prepared the data as well as how the package was installed and what version of the package are you using. When listing steps, don’t just say what you did, but explain how you did it.

\item {} 
Provide specific examples to demonstrate the steps. Include links to files or GitHub projects. If you’re providing snippets in the issue, use Markdown code blocks.

\item {} 
Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior.

\item {} 
Explain which behavior you expected to see instead and why.

\item {} 
If the problem is related to performance or memory, include a CPU profile capture with your report.

\end{itemize}

Provide more context by answering these questions:
\begin{itemize}
\item {} 
Did the problem start happening recently (e.g. after updating the version dependencies) or was this always a problem?

\item {} 
If the problem started happening recently, can you reproduce the problem in an older version? What’s the most recent version in which the problem doesn’t happen?

\item {} 
Can you reliably reproduce the issue? If not, provide details about how often the problem happens and under which conditions it normally happens.

\end{itemize}

Include details about your configuration and environment:
\begin{itemize}
\item {} 
Which version of spark-crowd are you using?

\item {} 
What’s the name and version of the OS you’re using?

\item {} 
Are you running using the package in a virtual machine? If so, which VM software are you using and which operating systems and versions are used for the host and the guest?

\end{itemize}


\section{Suggesting enhancements}
\label{\detokenize{package/contributors:suggesting-enhancements}}
We are open to suggestions of new features and minor improvements to existing functionality. Please follow the guidelines to help maintainers and the community
understand your suggestion. When requesting and enhancement please include as many details as possible.

Enhancement suggestions are tracked using Github Issues. To request an enhancement create an issue and provide the following information.
\begin{itemize}
\item {} 
Use a clear and descriptive title for the issue to identify the suggestion.

\item {} 
Provide a step-by-step description of the suggested enhancement in as many details as possible.

\item {} 
Provide specific examples to demonstrate the steps. Include copy/pasteable snippets which you use in those examples, as Markdown code blocks.

\item {} 
Describe the current behavior and explain which behavior you expected to see instead and why.

\item {} 
Explain why this enhancement would be useful to the users.

\item {} 
Specify which version of the package you’re using. Specify the name and version of the OS you’re using.

\end{itemize}


\section{New algorithms}
\label{\detokenize{package/contributors:new-algorithms}}
We are also grateful for contributions of new algorithms, as long as they improve the results or add new functionality to the ones existing in the package.
New algorithms must be published in peer-review publications for them to be considered. New algorithms must adhere to the architechture of this package and
should take into account the scalability of the learning process.

To contribute an algorithm first create a request using Github Issues, for the maintainers to review the suggestion. This request should provide the following information:
\begin{itemize}
\item {} 
Publication where the algorithm detais can be reviewed.

\item {} 
Explain why this algorithm would be useful to the users.

\end{itemize}

If the request is accepted, create a Github pull request with the new algorithm, as well as all necessary types to use it, so that the maintainers can review the
code and add it to the package.

Learning from crowdsourced data imposes new challenges
in the area of machine learning. \sphinxtitleref{spark-crowd} helps practitioners
when dealing with this kind of data at scale, using Apache Spark.

\noindent\sphinxincludegraphics{{illustration}.png}

The main features of \sphinxtitleref{spark-crowd} are the following:
\begin{itemize}
\item {} 
It implements well-known methods for learning from crowdsourced labeled data.

\item {} 
It is suitable for working with both large and small datasets.

\item {} 
It uses Apache Spark, which allows the code to run in different environments, from a computer to a multi-node cluster.

\item {} 
It is suitable both for research and production environments.

\item {} 
It provides an easy to use API, allowing the practitioner to start using the library in minutes.

\end{itemize}

See the {\hyperref[\detokenize{usage/quickstart:quickstart}]{\sphinxcrossref{\DUrole{std,std-ref}{Quick Start}}}} to get started using \sphinxtitleref{spark-crowd}

\sphinxtitleref{spark-crowd} is licensed under the \sphinxhref{https://opensource.org/licenses/MIT/}{MIT license}.



\renewcommand{\indexname}{Index}
\printindex
\end{document}